# Papernotes
<!-- 2021-07, Wentao Zhang, NIPS2021, RIM: Reliable Influence-based Active Learning on Graphs -->
<!-- 2021-07, Tianyang Lin, twitter -->
<!-- 2021-04， Wentao Zhang, https://machinelearning.apple.com/updates/apple-scholars-aiml-2021 -->
<!-- 2021-04， Xingyi Zhou, Facebook 2021 PhD Fellowship -->
<!-- 2020-ACL, FNLP: Ming Zhong + Danqing Wang + Zhan Shi + Xiaonan Li -->
<!-- 2020-04， Xingyi Zhou + Dequan Wang， https://github.com/xingyizhou/CenterNet Star:4.3K -->
<!-- 2019-04， 郑哲东， CVPR2019（Oral） -->
#### 2022-04
- Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [[arXiv](https://arxiv.org/abs/2204.01691)]

#### 2022-03
- Self-Consistency Improves Chain of Thought Reasoning in Language Models [[arXiv](https://arxiv.org/abs/2203.11171)]
- Exploring Plain Vision Transformer Backbones for Object Detection [[arXiv](https://arxiv.org/abs/2203.16527)]
- Pathways: Asynchronous Distributed Dataflow for ML [[arXiv](https://arxiv.org/abs/2203.12533)]
- DeepNet: Scaling Transformers to 1,000 Layers [[arXiv](https://arxiv.org/abs/2203.00555)] [[Code](https://github.com/microsoft/unilm)]

#### ICLR-22
- Bootstrapped Meta-Learning [[arXiv](https://arxiv.org/abs/2109.04504)]
- Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path [[arXiv](https://arxiv.org/abs/2106.02073)]
- Expressiveness and Approximation Properties of Graph Neural Networks [[arXiv](https://arxiv.org/abs/2204.04661)]
- Learning strides in convolutional neural networks [[arXiv](https://arxiv.org/abs/2202.01653)]
- Hyperparameter Tuning with Renyi Differential Privacy [[arXiv](https://arxiv.org/abs/2110.03620)]

#### 2022-02
- Overcoming a Theoretical Limitation of Self-Attention [[arXiv](https://arxiv.org/abs/2202.12172)]
- Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework [[arXiv](https://arxiv.org/abs/2202.07123)] [[Code](https://github.com/ma-xu/pointMLP-pytorch)]
- Transformer Quality in Linear Time [[arXiv](https://arxiv.org/abs/2202.10447)]
- Transformer Memory as a Differentiable Search Index [[arXiv](https://arxiv.org/abs/2202.06991)]
- DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers [[arXiv](https://arxiv.org/abs/2202.04053)] [[Code](https://github.com/j-min/DallEval)]
- Locating and Editing Factual Knowledge in GPT [[arXiv](https://arxiv.org/abs/2202.05262)] [[Code](https://github.com/kmeng01/rome)]
- SGPT: GPT Sentence Embeddings for Semantic Search [[arXiv](https://arxiv.org/abs/2202.08904)] [[Code](https://github.com/Muennighoff/sgpt)]
- MaskGIT: Masked Generative Image Transformer [[arXiv](https://arxiv.org/abs/2202.04200)]
- Diversify and Disambiguate: Learning From Underspecified Data [[arXiv](https://arxiv.org/abs/2202.03418)] 
- How to Understand Masked Autoencoders [[arXiv](https://arxiv.org/abs/2202.03670)]
- Understanding Contrastive Learning Requires Incorporating Inductive Biases [[arXiv](https://arxiv.org/abs/2202.14037)] 
- How Do Vision Transformers Work? [[arXiv](https://arxiv.org/abs/2202.06709)] [[Code](https://github.com/xxxnell/how-do-vits-work)]
- Gradients without Backpropagation [[arXiv](https://arxiv.org/abs/2202.08587)]
- Visual Attention Network [[arXiv](https://arxiv.org/abs/2202.09741)] [[Code](https://github.com/Visual-Attention-Network/VAN-Classification)]

#### 2022-01
- LaMDA: Language Models for Dialog Applications [[arXiv](https://arxiv.org/abs/2201.08239)]
- A ConvNet for the 2020s [[arXiv](https://arxiv.org/abs/2201.03545)] [[Code](https://github.com/facebookresearch/ConvNeXt)]

#### 2021-12
- Masked Feature Prediction for Self-Supervised Visual Pre-Training [[arXiv](https://arxiv.org/abs/2112.09133)]
- Are Large-scale Datasets Necessary for Self-Supervised Pre-training? [[arXiv](https://arxiv.org/abs/2112.10740)]
- Improving language models by retrieving from trillions of tokens [[arXiv](https://arxiv.org/abs/2112.04426)]

#### NIPS-22
- ATOM3D: Tasks On Molecules in Three Dimensions [[arXiv](https://arxiv.org/abs/2012.04035)]
- Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research [[arXiv](https://arxiv.org/abs/2112.01716)]
- Moser Flow: Divergence-based Generative Modeling on Manifolds [[arXiv](https://arxiv.org/abs/2108.08052)]
- A Continuized View on Nesterov Acceleration for Stochastic Gradient Descent and Randomized Gossip [[arXiv](https://arxiv.org/abs/2106.07644)]
- MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers [[arXiv](https://arxiv.org/abs/2102.01454)]
- Deep Reinforcement Learning at the Edge of the Statistical Precipice [[arXiv](https://arxiv.org/abs/2108.13264)]
- On the Expressivity of Markov Reward [[arXiv](https://arxiv.org/abs/2111.00876)]
- A Universal Law of Robustness via Isoperimetry [[arXiv](https://arxiv.org/abs/2105.12806)]

#### 2021-11
- Sparse is Enough in Scaling Transformers [[arXiv](https://arxiv.org/abs/2111.12763)]
- ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning [[arXiv](https://arxiv.org/abs/2111.10952)]
- A Survey of Generalisation in Deep Reinforcement Learning [[arXiv](https://arxiv.org/abs/2111.09794)]
- Data Augmentation Can Improve Robustness [[arXiv](https://arxiv.org/abs/2111.05328)]
- Gradients are Not All You Need [[arXiv](https://arxiv.org/abs/2111.05803)]
- Are Transformers More Robust Than CNNs? [[arXiv](https://arxiv.org/abs/2111.05464)] [[Code](https://github.com/ytongbai/ViTs-vs-CNNs)]
- Florence: A New Foundation Model for Computer Vision [[arXiv](https://arxiv.org/abs/2111.11432)]
- Masked Autoencoders Are Scalable Vision Learners [[arXiv](https://arxiv.org/abs/2111.06377)]
- Projected GANs Converge Faster [[arXiv](https://arxiv.org/abs/2111.01007)]

#### 2021-10
- NormFormer: Improved Transformer Pretraining with Extra Normalization [[arXiv](https://arxiv.org/abs/2110.09456)] [[Code](https://github.com/pytorch/fairseq/tree/main/examples/normformer)]
- VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization [[arXiv](https://arxiv.org/abs/2110.14363)]
- Similarity and Matching of Neural Network Representations [[arXiv](https://arxiv.org/abs/2110.14633)]
- Fast Model Editing at Scale [[arXiv](https://arxiv.org/abs/2110.11309)] [[Code](https://github.com/eric-mitchell/mend)]
- Understanding How Encoder-Decoder Architectures Attend [[arXiv](https://arxiv.org/abs/2110.15253)]
- ADOP: Approximate Differentiable One-Pixel Point Rendering [[arXiv](https://arxiv.org/abs/2110.06635)] [[Code](https://github.com/darglein/ADOP)]
- Learning in High Dimension Always Amounts to Extrapolation [[arXiv](https://arxiv.org/abs/2110.09485)]

#### 2021-09
- Primer: Searching for Efficient Transformers for Language Modeling [[arXiv](https://arxiv.org/abs/2109.08668)] [[Code](https://github.com/google-research/google-research/tree/master/primer)]
- Datasets: A Community Library for Natural Language Processing [[arXiv](https://arxiv.org/abs/2109.02846)] [[Code](https://github.com/huggingface/datasets)]
- Finetuned Language Models Are Zero-Shot Learners [[arXiv](https://arxiv.org/abs/2109.01652)]
- ∞-former: Infinite Memory Transformer [[arXiv](https://arxiv.org/abs/2109.00301)]

#### 2021-08
- Multi-Task Self-Training for Learning General Representations [[arXiv](https://arxiv.org/abs/2108.11353)]
- Fastformer: Additive Attention Can Be All You Need [[arXiv](https://arxiv.org/abs/2108.09084)] [[Code](https://github.com/wuch15/Fastformer)]
- On the Opportunities and Risks of Foundation Models [[arXiv](https://arxiv.org/abs/2108.07258)]

#### 2021-07
- Combiner: Full Attention Transformer with Sparse Computation Cost [[arXiv](https://arxiv.org/abs/2107.05768)]
- Long-Short Transformer: Efficient Transformers for Language and Vision [[arXiv](https://arxiv.org/abs/2107.02192)] [[Code](https://github.com/NVIDIA/transformer-ls)]
- Per-Pixel Classification is Not All You Need for Semantic Segmentation [[arXiv](https://arxiv.org/abs/2107.06278)] [[Code](https://github.com/facebookresearch/MaskFormer)]

#### ICML-21
- Understanding self-supervised Learning Dynamics without Contrastive Pairs [[arXiv](https://arxiv.org/abs/2102.06810)] [[Code](https://github.com/facebookresearch/luckmatters/tree/master/ssl)]
- Oops I Took A Gradient: Scalable Sampling for Discrete Distributions [[arXiv](https://arxiv.org/abs/2102.04509)]
- ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases [[arXiv](https://arxiv.org/abs/2103.10697)] [[Code](https://github.com/facebookresearch/convit)]

#### 2021-06
- Towards Understanding and Mitigating Social Biases in Language Models [[arXiv](https://arxiv.org/abs/2106.13219)] [[Code](https://github.com/pliang279/LM_bias)]
- Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer [[arXiv](https://arxiv.org/abs/2106.16171)]
- Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks [[arXiv](https://arxiv.org/abs/2106.04489)] [[Code](https://github.com/rabeehk/hyperformer)]
- Compacter: Efficient Low-Rank Hypercomplex Adapter Layers [[arXiv](https://arxiv.org/abs/2106.04647)] [[Code](https://github.com/rabeehk/compacter/)]
- Charformer: Fast Character Transformers via Gradient-based Subword Tokenization [[arXiv](https://arxiv.org/abs/2106.12672)] [[Code](https://github.com/google-research/google-research/tree/master/charformer)]
- A Survey of Transformers [[arXiv](https://arxiv.org/abs/2106.04554)]
- The Principles of Deep Learning Theory [[arXiv](https://arxiv.org/abs/2106.10165)]
- How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers [[arXiv](https://arxiv.org/abs/2106.10270)]
- SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption [[arXiv](https://arxiv.org/abs/2106.15147)]
- Early Convolutions Help Transformers See Better [[arXiv](https://arxiv.org/abs/2106.14881)]
- Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data [[arXiv](https://arxiv.org/abs/2106.11189)]
- Scaling Vision Transformers [[arXiv](https://arxiv.org/abs/2106.04560)]

#### 2021-05
- ByT5: Towards a token-free future with pre-trained byte-to-byte models [[arXiv](https://arxiv.org/abs/2105.13626)] [[Code](https://github.com/google-research/byt5)]
- Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level [[arXiv](https://arxiv.org/abs/2105.06020)] [[Code](https://github.com/ruiqi-zhong/acl2021-instance-level)]
- Aggregating Nested Transformers [[arXiv](https://arxiv.org/abs/2105.12723)]
- Diffusion Models Beat GANs on Image Synthesis [[arXiv](https://arxiv.org/abs/2105.05233)] [[Code](https://arxiv.org/abs/2105.05233)]
- The Modern Mathematics of Deep Learning [[arXiv](https://arxiv.org/abs/2105.04026)]
- Pay Attention to MLPs [[arXiv](https://arxiv.org/abs/2105.08050)]
- FNet: Mixing Tokens with Fourier Transforms [[arXiv](https://arxiv.org/abs/2105.03824)]

#### 2021-04
- Generating Datasets with Pretrained Language Models [[arXiv](https://arxiv.org/abs/2104.07540)] [[Code](https://github.com/timoschick/dino)]
- Retrieval Augmentation Reduces Hallucination in Conversation [[arXiv](https://arxiv.org/abs/2104.07567)] 
- Is Your Language Model Ready for Dense Representation Fine-tuning? [[arXiv](https://arxiv.org/abs/2104.08253)] [[Code](https://github.com/luyug/Condenser)]
- On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies [[arXiv](https://arxiv.org/abs/2104.05694)]
- Multiscale Vision Transformers [[arXiv](https://arxiv.org/abs/2104.11227)] [[Code](https://github.com/facebookresearch/SlowFast)]
- Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study [[arXiv](https://arxiv.org/abs/2104.00676)]
- An Empirical Study of Training Self-Supervised Vision Transformers [[arXiv](https://arxiv.org/abs/2104.02057)]
- EfficientNetV2: Smaller Models and Faster Training [[arXiv](https://arxiv.org/abs/2104.00298)] [[Code](https://github.com/google/automl/efficientnetv2)]

#### 2021-03
- GPT Understands, Too [[arXiv](https://arxiv.org/abs/2103.10385)] [[Code](https://github.com/THUDM/P-tuning)]
- StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery [[arXiv](https://arxiv.org/abs/2103.17249)] [[Code](https://github.com/orpatashnik/StyleCLIP)]
- Deep learning: a statistical viewpoint [[arXiv](https://arxiv.org/abs/2103.09177)]
- CvT: Introducing Convolutions to Vision Transformers [[arXiv](https://arxiv.org/abs/2103.15808)] [[Code](https://github.com/leoxiaobin/CvT)]
- Vision Transformers for Dense Prediction [[arXiv](https://arxiv.org/abs/2103.13413)] [[Code](https://github.com/intel-isl/DPT)]
- Swin Transformer: Hierarchical Vision Transformer using Shifted Windows [[arXiv](https://arxiv.org/abs/2103.14030)] [[Code](https://github.com/microsoft/Swin-Transformer)]
- Revisiting ResNets: Improved Training and Scaling Strategies [[arXiv](https://arxiv.org/abs/2103.07579)]
- OmniNet: Omnidirectional Representations from Transformers [[arXiv](https://arxiv.org/abs/2103.01075)]
- Pretrained Transformers as Universal Computation Engines [[arXiv](https://arxiv.org/abs/2103.05247)] [[Code](https://github.com/kzl/universal-computation)]
- Generative Adversarial Transformers [[arXiv](https://arxiv.org/abs/2103.01209)] [[Code](https://github.com/dorarad/gansformer)]
- Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models [[arXiv](https://arxiv.org/abs/2103.04922)]
- Generating Images with Sparse Representations [[arXiv](https://arxiv.org/abs/2103.03841)]
- Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth [[arXiv](https://arxiv.org/abs/2103.03404)] [[Code](https://github.com/twistedcubic/attention-rank-collapse)]
- Coordination Among Neural Modules Through a Shared Global Workspace [[arXiv](https://arxiv.org/abs/2103.01197)]
- Transformers with Competitive Ensembles of Independent Mechanisms [[arXiv](https://arxiv.org/abs/2103.00336)]
- Self-supervised Pretraining of Visual Features in the Wild [[arXiv](https://arxiv.org/abs/2103.01988)] [[Code](https://github.com/facebookresearch/vissl)]

#### ICLR-21
- Score-Based Generative Modeling through Stochastic Differential Equations [[arXiv](https://arxiv.org/abs/2011.13456)]
- Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime [[arXiv](https://arxiv.org/abs/2006.12297)]
- Complex Query Answering with Neural Link Predictors [[arXiv](https://arxiv.org/abs/2011.03459)]

#### 2021-02
- Linear Transformers Are Secretly Fast Weight Memory Systems [[arXiv](https://arxiv.org/abs/2102.11174)] [[Code](https://github.com/ischlag/fast-weight-transformers)]
- Is Space-Time Attention All You Need for Video Understanding? [[arXiv](https://arxiv.org/abs/2102.05095)]
- Formal Language Theory Meets Modern NLP [[arXiv](https://arxiv.org/abs/2102.10094)]
- LambdaNetworks: Modeling Long-Range Interactions Without Attention [[arXiv](https://arxiv.org/abs/2102.08602)]
- Do Transformer Modifications Transfer Across Implementations and Applications? [[arXiv](https://arxiv.org/abs/2102.11972)] [[Code](https://github.com/google-research/google-research/tree/master/transformer_modifications)]
- Adaptive Semiparametric Language Models [[arXiv](https://arxiv.org/abs/2102.02557)]
- Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer [[arXiv](https://arxiv.org/abs/2102.10772)]
- Towards Causal Representation Learning [[arXiv](https://arxiv.org/abs/2102.11107)]
- Zero-Shot Text-to-Image Generation [[arXiv](https://arxiv.org/abs/2102.12092)]
- High-Performance Large-Scale Image Recognition Without Normalization [[arXiv](https://arxiv.org/abs/2102.06171)] [[Code](https://github.com/deepmind/deepmind-research/tree/master/nfnets)]

#### 2021-01
- Do We Really Need Deep Learning Models for Time Series Forecasting? [[arXiv](https://arxiv.org/abs/2101.02118)]
- Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing [[arXiv](https://arxiv.org/abs/2101.03289)] [[Code](https://github.com/nlp-uoregon/trankit)]
- On the Calibration and Uncertainty of Neural Learning to Rank Models [[arXiv](https://arxiv.org/abs/2101.04356)] [[Code](https://github.com/Guzpenha/transformer_rankers/tree/uncertainty_estimation)]
- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity [[arXiv](https://arxiv.org/abs/2101.03961)] [[Code](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)]
- AutoDropout: Learning Dropout Patterns to Regularize Deep Networks [[arXiv](https://arxiv.org/abs/2101.01761)]

#### 2020-12
- Training data-efficient image transformers & distillation through attention [[arXiv](https://arxiv.org/abs/2012.12877)] [[Code](https://github.com/facebookresearch/deit)]
- Deep Learning-Based Human Pose Estimation: A Survey [[arXiv](https://arxiv.org/abs/2012.13392)] [[Code](https://github.com/zczcwh/DL-HPE)]
- Global Context Networks [[arXiv](https://arxiv.org/abs/2012.13375)] [[Code](https://github.com/xvjiarui/GCNet)]
- RealFormer: Transformer Likes Residual Attention [[arXiv](https://arxiv.org/abs/2012.11747)]
- Point Transformer [[arXiv](https://arxiv.org/abs/2012.09164)]

#### NIPS-20
- Improved guarantees and a multiple-descent curve for Column Subset Selection and the Nyström method [[arXiv](https://arxiv.org/abs/2002.09073)]
- No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium [[arXiv](https://arxiv.org/abs/2004.00603)]

#### 2020-11
- FP-NAS: Fast Probabilistic Neural Architecture Search [[arXiv](https://arxiv.org/abs/2011.10949)]
- Topology of Word Embeddings: Singularities Reflect Polysemy [[arXiv](https://arxiv.org/abs/2011.09413)]
- Language Model is All You Need: Natural Language Understanding as Question Answering [[arXiv](https://arxiv.org/abs/2011.03023)]
- Ridge Rider: Finding Diverse Solutions by Following Eigenvectors of the Hessian [[arXiv](https://arxiv.org/abs/2011.06505)]
- FROST: Faster and more Robust One-shot Semi-supervised Training [[arXiv](https://arxiv.org/abs/2011.09471)] [[Code](https://github.com/HelenaELiu/FROST)]
- Teaching with Commentaries [[arXiv](https://arxiv.org/abs/2011.03037)]
- Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning [[arXiv](https://arxiv.org/abs/2011.10043)]
- A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges [[arXiv](https://arxiv.org/abs/2011.06225)]
- Intriguing Properties of Contrastive Losses [[arXiv](https://arxiv.org/abs/2011.02803)] [[Code](https://github.com/google-research/simclr/tree/master/colabs/intriguing_properties)]
- Exploring Simple Siamese Representation Learning [[arXiv](https://arxiv.org/abs/2011.10566)]
- Long Range Arena: A Benchmark for Efficient Transformers [[arXiv](https://arxiv.org/abs/2011.04006)] [[Code](https://github.com/google-research/long-range-arena)]
- Learning Latent Representations to Influence Multi-Agent Interaction [[arXiv](https://arxiv.org/abs/2011.06619)]

#### 2020-10
- A Frustratingly Easy Approach for Joint Entity and Relation Extraction [[arXiv](https://arxiv.org/abs/2010.12812)]
- Human-centric Dialog Training via Offline Reinforcement Learning [[arXiv](https://arxiv.org/abs/2010.05848)] [[Code](https://github.com/natashamjaques/neural_chat)]
- Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervisio [[arXiv](https://arxiv.org/abs/2010.06775)] [[Code](https://github.com/airsplay/vokenization)]
- Self-training Improves Pre-training for Natural Language Understanding [[arXiv](https://arxiv.org/abs/2010.02194)]
- Representation Learning via Invariant Causal Mechanisms [[arXiv](https://arxiv.org/abs/2010.07922)]
- Human-centric Dialog Training via Offline Reinforcement Learning [[arXiv](https://arxiv.org/abs/2010.05848)]
- Neural Databases [[arXiv](https://arxiv.org/abs/2010.06973)]
- Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking [[arXiv](https://arxiv.org/abs/2010.00577)]
- Self-training Improves Pre-training for Natural Language Understanding [[arXiv](https://arxiv.org/abs/2010.02194)]
- CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning [[arXiv](https://arxiv.org/abs/2010.04296)]
- A Survey of Deep Meta-Learning [[arXiv](https://arxiv.org/abs/2010.03522)] 
- AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients [[arXiv](https://arxiv.org/abs/2010.07468)] [[Code](https://github.com/juntang-zhuang/Adabelief-Optimizer)]

#### 2020-09
- X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers [[arXiv](https://arxiv.org/abs/2009.11278)]
- Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves [[arXiv](https://arxiv.org/abs/2009.11243)]
- A Unifying Review of Deep and Shallow Anomaly Detection [[arXiv](https://arxiv.org/abs/2009.11732)]
- Rethinking Attention with Performers [[arXiv](https://arxiv.org/abs/2009.14794)] [[Code](https://github.com/google-research/google-research/tree/master/performer/fast_self_attention)]
- Principles and Practice of Explainable Machine Learning [[arXiv](https://arxiv.org/abs/2009.11698)]
- Efficient Transformers: A Survey [[arXiv](https://arxiv.org/abs/2009.06732)]
- Generative Language Modeling for Automated Theorem Proving [[arXiv](https://arxiv.org/abs/2009.03393)]

#### 2020-08
- DeLighT: Very Deep and Light-weight Transformer [[arXiv](https://arxiv.org/abs/2008.00623)] [[Code](https://github.com/sacmehta/delight)]
- Deep Search Query Intent Understanding [[arXiv](https://arxiv.org/abs/2008.06759)]
- Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries [[arXiv](https://arxiv.org/abs/2008.09036)]
- Offline Meta-Reinforcement Learning with Advantage Weighting [[arXiv](https://arxiv.org/abs/2008.06043)]
- Compression of Deep Learning Models for Text: A Survey [[arXiv](https://arxiv.org/abs/2008.05221)]
- learn2learn: A Library for Meta-Learning Research [[arXiv](https://arxiv.org/abs/2008.12284)] [[Code](https://github.com/learnables/learn2learn)]
- Training Deep Neural Networks Without Batch Normalization [[arXiv](https://arxiv.org/abs/2008.07970)]
- The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement [[arXiv](https://arxiv.org/abs/2008.10599)] [[Code](https://github.com/wpeebles/hessian_penalty)]
- The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models [[arXiv](https://arxiv.org/abs/2008.05122)] [[Code](https://github.com/PAIR-code/lit)]
- Hopfield Networks is All You Need [[arXiv](https://arxiv.org/abs/2008.02217)] [[Code](https://github.com/ml-jku/hopfield-layers)]

#### 2020-07
- Finite Versus Infinite Neural Networks: an Empirical Study [[arXiv](https://arxiv.org/abs/2007.15801)]
- Graph Structure of Neural Networks [[arXiv](https://arxiv.org/abs/2007.06559)]
- Towards Learning Convolutions from Scratch [[arXiv](https://arxiv.org/abs/2007.13657)]
- The Lottery Ticket Hypothesis for Pre-trained BERT Networks [[arXiv](https://arxiv.org/abs/2007.12223)] [[Code](https://github.com/TAMU-VITA/BERT-Tickets)]
- Do Transformers Need Deep Long-Range Memory [[arXiv](https://arxiv.org/abs/2007.03356)]
- RetrieveGAN: Image Synthesis via Differentiable Patch Retrieval [[arXiv](https://arxiv.org/abs/2007.08513)]
- Contrastive Learning for Unpaired Image-to-Image Translation [[arXiv](https://arxiv.org/abs/2007.15651)] [[Code](https://github.com/taesungp/contrastive-unpaired-translation)]
- Do Adversarially Robust ImageNet Models Transfer Better? [[arXiv](https://arxiv.org/abs/2007.08489)] [[Code](https://github.com/Microsoft/robust-models-transfer)]
- Contrastive Code Representation Learning [[arXiv](https://arxiv.org/abs/2007.04973)] [[Code](https://github.com/parasj/contracode)]
- Strong Generalization and Efficiency in Neural Programs [[arXiv](https://arxiv.org/abs/2007.03629)] 
- Accelerating 3D Deep Learning with PyTorch3D [[arXiv](https://arxiv.org/abs/2007.08501)]
- Graph Structure of Neural Networks [[arXiv](https://arxiv.org/abs/2007.06559)]
- Big Bird: Transformers for Longer Sequences [[arXiv](https://arxiv.org/abs/2007.14062)]
- NVAE: A Deep Hierarchical Variational Autoencoder [[arXiv](https://arxiv.org/abs/2007.03898)]
- Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval [[arXiv](https://arxiv.org/abs/2007.00808)] [[Code](https://github.com/microsoft/ANCE)]
- Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge [[arXiv](https://arxiv.org/abs/2007.00849)]
- Language-agnostic BERT Sentence Embedding [[arXiv](https://arxiv.org/abs/2007.01852)]
- Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning [[arXiv](https://arxiv.org/abs/2007.01293)]

#### 2020-06
- Embedding-based Retrieval in Facebook Search [[arXiv](https://arxiv.org/abs/2006.11632)]
- Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction [[arXiv](https://arxiv.org/abs/2006.05639)]
- Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains [[arXiv](https://arxiv.org/abs/2006.10739)]
- Implicit Neural Representations with Periodic Activation Functions [[arXiv](https://arxiv.org/abs/2006.09661)] [[Code](https://github.com/vsitzmann/siren)]
- Big Self-Supervised Models are Strong Semi-Supervised Learners [[arXiv](https://arxiv.org/abs/2006.10029)] [[Code](https://github.com/google-research/simclr)]
- Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention [[arXiv](https://arxiv.org/abs/2006.16236)]
- Acme: A Research Framework for Distributed Reinforcement Learning [[arXiv](https://arxiv.org/abs/2006.00979)] [[Code](https://github.com/deepmind/acme)]

#### 2020-05
- End-to-End Object Detection with Transformers [[arXiv](https://arxiv.org/abs/2005.12872)] [[Code](https://github.com/facebookresearch/detr)]
- Jukebox: A Generative Model for Music [[arXiv](https://arxiv.org/abs/2005.00341)] [[Code](https://github.com/openai/jukebox)]
- Beyond Accuracy: Behavioral Testing of NLP models with CheckList [[arXiv](https://arxiv.org/abs/2005.04118)] [[Code](https://github.com/marcotcr/checklist)]
- Neural Controlled Differential Equations for Irregular Time Series [[arXiv](https://arxiv.org/abs/2005.08926)] [[Code](https://github.com/patrick-kidger/NeuralCDE)]
- Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors [[arXiv](https://arxiv.org/abs/2005.07186)] [[Code](https://github.com/google/edward2)]
- Language Models are Few-Shot Learners [[arXiv](https://arxiv.org/abs/2005.14165)]
- Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere [[arXiv](https://arxiv.org/abs/2005.10242)] [[Code](https://github.com/SsnL/align_uniform)]
- What makes for good views for contrastive learning [[arXiv](https://arxiv.org/abs/2005.10243)] [[Code](https://github.com/HobbitLong/PyContrast)]
- Planning to Explore via Self-Supervised World Models [[arXiv](https://arxiv.org/abs/2005.05960)]
- Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems [[arXiv](https://arxiv.org/abs/2005.01643)]
- Synthesizer: Rethinking Self-Attention in Transformer Models [[arXiv](https://arxiv.org/abs/2005.00743)] 

#### 2020-04
- The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies [[arXiv](https://arxiv.org/abs/2004.13332)] [[Code](https://github.com/salesforce/ai-economist)]
- NBDT: Neural-Backed Decision Trees [[arXiv](https://arxiv.org/abs/2004.00221)] [[Code](https://github.com/alvinwan/neural-backed-decision-trees)]
- YOLOv4: Optimal Speed and Accuracy of Object Detection [[arXiv](https://arxiv.org/abs/2004.10934)] [[Code](https://github.com/AlexeyAB/darknet)]
- ResNeSt: Split-Attention Networks [[arXiv](https://arxiv.org/abs/2004.08955)] [[Code](https://github.com/zhanghang1989/ResNeSt)]
- Evolving Normalization-Activation Layers [[arXiv](https://arxiv.org/abs/2004.02967)] [[Code](https://github.com/tensorflow/tpu/tree/master/models/official/resnet)]
- Longformer: The Long-Document Transformer [[arXiv](https://arxiv.org/abs/2004.05150)] [[Code](https://github.com/allenai/longformer)]
- Adversarial Latent Autoencoders [[arXiv](https://arxiv.org/abs/2004.04467)] [[Code](https://github.com/podgorskiy/ALAE)]
- Supervised Contrastive Learning [[arXiv](https://arxiv.org/abs/2004.11362)] [[Code](https://github.com/HobbitLong/SupContrast)]
- CURL: Contrastive Unsupervised Representations for Reinforcement Learning [[arXiv](https://arxiv.org/abs/2004.04136)] [[Code](https://github.com/MishaLaskin/curl)]

#### ICML-20
- Stabilizing Differentiable Architecture Search via Perturbation-based Regularization [[arXiv](https://arxiv.org/abs/2002.05283)] [[Code](https://github.com/xiangning-chen/SmoothDARTS)]
- On Learning Sets of Symmetric Elements [[arXiv](https://arxiv.org/abs/2002.08599)]
- AutoML-Zero: Evolving Machine Learning Algorithms From Scratch [[arXiv](https://arxiv.org/abs/2003.03384)] [[Code](https://github.com/google-research/google-research/tree/master/automl_zero#automl-zero)]

#### 2020-03
- XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization [[arXiv](https://arxiv.org/abs/2003.11080)] [[Code](https://github.com/google-research/xtreme)]
- Agent57: Outperforming the Atari Human Benchmark [[arXiv](https://arxiv.org/abs/2003.13350)]
- Talking-Heads Attention [[arXiv](https://arxiv.org/abs/2003.02436)]  
- ReZero is All You Need: Fast Convergence at Large Depth [[arXiv](https://arxiv.org/abs/2003.04887)] [[Code](https://github.com/majumderb/rezero)]
- AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data [[arXiv](https://arxiv.org/abs/2003.06505)] [[Code](https://github.com/awslabs/autogluon)]
- ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators [[arXiv](https://arxiv.org/abs/2003.10555)] [[Code](https://github.com/google-research/electra)]
- A Survey of Deep Learning for Scientific Discovery [[arXiv](https://arxiv.org/abs/2003.11755)]
- Designing Network Design Spaces [[arXiv](https://arxiv.org/abs/2003.13678)] [[Code](https://github.com/facebookresearch/pycls)] 

#### 2020-02
- On Feature Normalization and Data Augmentation [[arXiv](https://arxiv.org/abs/2002.11102)] [[Code](https://github.com/Boyiliee/MoEx)]
- Bayesian Deep Learning and a Probabilistic Perspective of Generalization [[arXiv](https://arxiv.org/abs/2002.08791)] [[Code](https://github.com/izmailovpavel/understandingbdl)]
- Self-Distillation Amplifies Regularization in Hilbert Space [[arXiv](https://arxiv.org/abs/2002.05715)]
- A Primer in BERTology: What we know about how BERT works [[arXiv](https://arxiv.org/abs/2002.12327)]
- Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping [[arXiv](https://arxiv.org/abs/2002.06305)]
- fastai: A Layered API for Deep Learning [[arXiv](https://arxiv.org/abs/2002.04688)] [[Code](https://github.com/fastai/fastai)]
- How Good is the Bayes Posterior in Deep Neural Networks Really? [[arXiv](https://arxiv.org/abs/2002.02405)] [[Code](https://github.com/google-research/google-research/tree/master/cold_posterior_bnn)]
- A Simple Framework for Contrastive Learning of Visual Representations [[arXiv](https://arxiv.org/abs/2002.05709)] [[Code](https://github.com/google-research/simclr)]

#### 2020-01
- Reformer: The Efficient Transformer [[arXiv](https://arxiv.org/abs/2001.04451)] [[Code](https://github.com/google/trax/tree/master/trax/models/reformer)]

#### 2019-12
- Neural Tangents: Fast and Easy Infinite Neural Networks in Python [[arXiv](https://arxiv.org/abs/1912.02803)] [[Code](https://github.com/google/neural-tangents)]
- Advances and Open Problems in Federated Learning [[arXiv](https://arxiv.org/abs/1912.04977)]
- PointRend: Image Segmentation as Rendering [[arXiv](https://arxiv.org/abs/1912.08193)] [[Code](https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend)]
- Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions [[arXiv](https://arxiv.org/abs/1912.02875)]
- Dota 2 with Large Scale Deep Reinforcement Learning [[arXiv](https://arxiv.org/abs/1912.06680)]

#### ICLR-20
- Residual Energy-Based Models for Text Generation [[arXiv](https://arxiv.org/abs/2004.11714)]
- Decoupling Representation and Classifier for Long-Tailed Recognition [[arXiv](https://arxiv.org/abs/1910.09217)] [[Code](https://github.com/facebookresearch/classifier-balancing)]
- Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One [[arXiv](https://arxiv.org/abs/1912.03263)]
- Neural Arithmetic Units [[arXiv](https://arxiv.org/abs/2001.05016)] [[Code](https://github.com/AndreasMadsen/stable-nalu)]
- Causal Discovery with Reinforcement Learning [[arXiv](https://arxiv.org/abs/1906.04477)] [[Code](https://github.com/huawei-noah/trustworthyAI/tree/master/Causal_Structure_Learning/Causal_Discovery_RL)]

#### 2019-11
- On the Relationship between Self-Attention and Convolutional Layers [[arXiv](https://arxiv.org/abs/1911.03584)] [[Code](https://github.com/epfml/attention-cnn/tree/arxiv-v1)]
- Single Headed Attention RNN: Stop Thinking With Your Head [[arXiv](https://arxiv.org/abs/1911.11423)] [[Code](https://github.com/smerity/sha-rnn)]
- Rigging the Lottery: Making All Tickets Winners [[arXiv](https://arxiv.org/abs/1911.11134)] [[Code](https://github.com/google-research/rigl)]
- Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model [[arXiv](https://arxiv.org/abs/1911.08265)]
- EfficientDet: Scalable and Efficient Object Detection [[arXiv](https://arxiv.org/abs/1911.09070)] [[Code](https://github.com/google/automl/tree/master/efficientdet)]
- Momentum Contrast for Unsupervised Visual Representation Learning [[arXiv](https://arxiv.org/abs/1911.05722)] [[Code](https://github.com/facebookresearch/moco)]
- Real-Time Reinforcement Learning [[arXiv](https://arxiv.org/abs/1911.04448)] [[Code](https://github.com/rmst/rtrl)]
- Self-training with Noisy Student improves ImageNet classification [[arXiv](https://arxiv.org/abs/1911.04252)] [[Code](https://github.com/google-research/noisystudent)]

#### 2019-10
- Specializing Word Embeddings (for Parsing) by Information Bottleneck [[arXiv](https://arxiv.org/abs/1910.00163)]
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension [[arXiv](https://arxiv.org/abs/1910.13461)]
- Contrastive Representation Distillation [[arXiv](https://arxiv.org/abs/1910.10699)] [[Code](https://github.com/HobbitLong/RepDistiller)]
- BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search [[arXiv](https://arxiv.org/abs/1910.11858)] [[Code](https://github.com/naszilla/bananas)]
- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [[arXiv](https://arxiv.org/abs/1910.10683)] [[Code](https://github.com/google-research/text-to-text-transfer-transformer)]
- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter [[arXiv](https://arxiv.org/abs/1910.01108)] [[Code](https://github.com/huggingface/transformers)]
- Generalized Inner Loop Meta-Learning [[arXiv](https://arxiv.org/abs/1910.01727)] [[Code](https://github.com/facebookresearch/higher)]

#### 2019-09
- Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT [[arXiv](https://arxiv.org/abs/1909.05840)]
- Meta-Learning with Implicit Gradients [[arXiv](https://arxiv.org/abs/1909.04630)]

#### NIPS-19
- Learning Disentangled Representations for Recommendation [[arXiv](https://arxiv.org/abs/1910.14238)] [[Code](https://jianxinma.github.io/disentangle-recsys.html)]
- Unified Language Model Pre-training for Natural Language Understanding and Generation [[arXiv](https://arxiv.org/abs/1905.03197)] [[Code](https://github.com/microsoft/unilm)]
- Putting An End to End-to-End: Gradient-Isolated Learning of Representations [[arXiv](https://arxiv.org/abs/1905.11786)] [[Code](https://github.com/loeweX/Greedy_InfoMax)]

#### 2019-08
- Once for All: Train One Network and Specialize it for Efficient Deployment [[arXiv](https://arxiv.org/abs/1908.09791)] [[Code](https://github.com/intel/mkl-dnn)]
- AutoML: A Survey of the State-of-the-Art [[arXiv](https://arxiv.org/abs/1908.00709)]
- On the Validity of Self-Attention as Explanation in Transformer Models [[arXiv](https://arxiv.org/abs/1908.04211)]
- Attention on Attention for Image Captioning [[arXiv](https://arxiv.org/abs/1908.06954)] [[Code](https://github.com/husthuaan/AoANet)]
- Attention is not not Explanation [[arXiv](https://arxiv.org/abs/1908.04626)] [[Code](https://github.com/sarahwie/attention)]
- The HSIC Bottleneck: Deep Learning without Back-Propagation [[arXiv](https://arxiv.org/abs/1908.01580)] [[Code](https://github.com/choasma/HSIC-bottleneck)]

#### 2019-07
- RoBERTa: A Robustly Optimized BERT Pretraining Approach [[arXiv](https://arxiv.org/abs/1907.11692)] [[Code](https://github.com/pytorch/fairseq)]
- Lookahead Optimizer: k steps forward, 1 step back [[arXiv](https://arxiv.org/abs/1907.08610)] [[Code](https://github.com/michaelrzhang/lookahead)] 
- OmniNet: A unified architecture for multi-modal multi-task learning [[arXiv](https://arxiv.org/abs/1907.07804)] [[Code](https://github.com/subho406/OmniNet)]
- Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches [[arXiv](https://arxiv.org/abs/1907.06902)] [[Code](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation)]
- A Differentiable Programming System to Bridge Machine Learning and Scientific Computing [[arXiv](https://arxiv.org/abs/1907.07587)] [[Code](https://github.com/MikeInnes/zygote-paper)]

#### 2019-06
- COMET: Commonsense Transformers for Automatic Knowledge Graph Construction [[arXiv](https://arxiv.org/abs/1906.05317)] [[Code](https://github.com/atcbosselut/comet-commonsense)]
- Deep Learning Recommendation Model for Personalization and Recommendation Systems [[arXiv](https://arxiv.org/abs/1906.00091)] [[Code](https://github.com/facebookresearch/dlrm)]
- Stacked Capsule Autoencoders [[arXiv](https://arxiv.org/abs/1906.06818)] [[Code](https://github.com/google-research/google-research/tree/master/stacked_capsule_autoencoders)]
- Is Attention Interpretable? [[arXiv](https://arxiv.org/abs/1906.03731)] [[Code](https://github.com/serrano-s/attn-tests)]
- Generating Diverse High-Fidelity Images with VQ-VAE-2 [[arXiv](https://arxiv.org/abs/1906.00446)]
- Visualizing and Measuring the Geometry of BERT [[arXiv](https://arxiv.org/abs/1906.02715)]
- Modern Deep Reinforcement Learning Algorithms [[arXiv](https://arxiv.org/abs/1906.10025)] [[Code](https://github.com/FortsAndMills/Learning-Reinforcement-Learning/tree/master/LRL)]
- When Does Label Smoothing Help? [[arXiv](https://arxiv.org/abs/1906.02629)]
- Large Scale Structure of Neural Network Loss Landscapes [[arXiv](https://arxiv.org/abs/1906.04724)]
- Generative Adversarial Networks: A Survey and Taxonomy [[arXiv](https://arxiv.org/abs/1906.01529)] [[Code](https://github.com/sheqi/GAN_Review)]
- Weight Agnostic Neural Networks [[arXiv](https://arxiv.org/abs/1906.04358)]
- An Introduction to Variational Autoencoders [[arXiv](https://arxiv.org/abs/1906.02691)]
- XLNet: Generalized Autoregressive Pretraining for Language Understanding [[arXiv](https://arxiv.org/abs/1906.08237)] [[Code](https://github.com/zihangdai/xlnet)]
- Selfie: Self-supervised Pretraining for Image Embedding [[arXiv](https://arxiv.org/abs/1906.02940)] 

#### ACL-19
- ERNIE: Enhanced Language Representation with Informative Entities [[arXiv](https://arxiv.org/abs/1905.07129)] [[Code](https://github.com/thunlp/ERNIE)]
- Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned [[arXiv](https://arxiv.org/abs/1905.09418)] [[Code](https://github.com/lena-voita/the-story-of-heads)]
- PaperRobot: Incremental Draft Generation of Scientific Ideas [[arXiv](https://arxiv.org/abs/1905.07870)] [[Code](https://github.com/EagleW/PaperRobot)]
- BERT Rediscovers the Classical NLP Pipeline [[arXiv](https://arxiv.org/abs/1905.05950)]

#### ICML-19
- Bayesian Generative Active Deep Learning [[arXiv](https://arxiv.org/abs/1904.11643)]
- Rates of Convergence for Sparse Variational Gaussian Process Regression [[arXiv](https://arxiv.org/abs/1903.03571)]
- Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations [[arXiv](https://arxiv.org/abs/1811.12359)] [[Code](https://github.com/google-research/disentanglement_lib)]
- On Variational Bounds of Mutual Information [[arXiv](https://arxiv.org/abs/1905.06922)]
- Making Convolutional Networks Shift-Invariant Again [[arXiv](https://arxiv.org/abs/1904.11486)] [[Code](https://github.com/adobe/antialiased-cnns)]
- Similarity of Neural Network Representations Revisited [[arXiv](https://arxiv.org/abs/1905.00414)]
- MASS: Masked Sequence to Sequence Pre-training for Language Generation [[arXiv](https://arxiv.org/abs/1905.02450)] [[Code]()]

#### 2019-05
- Deep Session Interest Network for Click-Through Rate Prediction [[arXiv](https://arxiv.org/abs/1905.06482)] [[Code](https://github.com/shenweichen/DSIN)]
- MixMatch: A Holistic Approach to Semi-Supervised Learning [[arXiv](https://arxiv.org/abs/1905.02249)] [[Code](https://github.com/google-research/mixmatch)]
- An Explicitly Relational Neural Network Architecture [[arXiv](https://arxiv.org/abs/1905.10307)]
- A Survey on Neural Architecture Search [[arXiv](https://arxiv.org/abs/1905.01392)]
- Learning What and Where to Transfer [[arXiv](https://arxiv.org/abs/1905.05901)]
- Object Detection in 20 Years: A Survey [[arXiv](https://arxiv.org/abs/1905.05055)]
- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks [[arXiv](https://arxiv.org/abs/1905.11946)] [[Code](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)]
- Adversarial Examples Are Not Bugs, They Are Features [[arXiv](https://arxiv.org/abs/1905.02175)]

#### 2019-04
- Multi-Interest Network with Dynamic Routing for Recommendation at Tmall [[arXiv](https://arxiv.org/abs/1904.08030)]
- Unsupervised Data Augmentation [[arXiv](https://arxiv.org/abs/1904.12848)] [[Code](https://github.com/google-research/uda)]
- A Closer Look at Few-shot Classification [[arXiv](https://arxiv.org/abs/1904.04232)] [[Code](https://github.com/wyharveychen/CloserLookFewShot)]
- Attention Augmented Convolutional Networks [[arXiv](https://arxiv.org/abs/1904.09925)]
- Survey on Automated Machine Learning [[arXiv](https://arxiv.org/abs/1904.12054)]
- Unifying Human and Statistical Evaluation for Natural Language Generation [[arXiv](https://arxiv.org/abs/1904.02792)] [[Code](https://github.com/hughbzhang/HUSE)]
- Generating Long Sequences with Sparse Transformers [[arXiv](https://arxiv.org/abs/1904.10509)] [[Code](https://github.com/openai/sparse_attention)]
- Neural Message Passing for Multi-Label Classification [[arXiv](https://arxiv.org/abs/1904.08049)] [[Code](https://github.com/QData/LaMP)]
- CNM: An Interpretable Complex-valued Network for Matching [[arXiv](https://arxiv.org/abs/1904.05298)] [[Code](https://github.com/wabyking/qnn)]
- What's in a Name? Reducing Bias in Bios without Access to Protected Attributes [[arXiv](https://arxiv.org/abs/1904.05233)]
- Analysing Mathematical Reasoning Abilities of Neural Models [[arXiv](https://arxiv.org/abs/1904.01557)] [[dataset](https://github.com/deepmind/mathematics_dataset)]

#### 2019-03
- To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks [[arXiv](https://arxiv.org/abs/1903.05987)]
- Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition [[arXiv](https://arxiv.org/abs/1903.10346)]
- Semantic Image Synthesis with Spatially-Adaptive Normalization [[arXiv](https://arxiv.org/abs/1903.07291)] [[Code](https://github.com/NVlabs/SPADE)]
- High-Fidelity Image Generation With Fewer Labels [[arXiv](https://arxiv.org/abs/1903.02271)] [[Code](https://github.com/google/compare_gan)]

#### NAACL-19
- CNM: An Interpretable Complex-valued Network for Matching [[arXiv](https://arxiv.org/abs/1904.05298)] [[Code](https://github.com/wabyking/qnn)]

#### 2019-02 
- A Simple Baseline for Bayesian Uncertainty in Deep Learning [[arXiv](https://arxiv.org/abs/1902.02476)] [[Code](https://github.com/wjmaddox/swa_gaussian)]
- Attention is not Explanation [[arXiv](https://arxiv.org/abs/1902.10186)] [[Code](https://github.com/successar/AttentionExplanation)]
- End-to-End Open-Domain Question Answering with BERTserini [[arXiv](https://arxiv.org/abs/1902.01718)]
- Embodied Multimodal Multitask Learning [[arXiv](https://arxiv.org/abs/1902.01385)]
- An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models [[arXiv](https://arxiv.org/abs/1902.10547)]
- Simplifying Graph Convolutional Networks [[arXiv](https://arxiv.org/abs/1902.07153)] [[Code](https://github.com/Tiiiger/SGC)]
- Superposition of many models into one [[arXiv](https://arxiv.org/abs/1902.05522)]
- Topology of Learning in Artificial Neural Networks [[arXiv](https://arxiv.org/abs/1902.08160)] [[Code](https://github.com/maximevictor/topo-learning)]
- A novel adaptive learning rate scheduler for deep neural networks [[arXiv](https://arxiv.org/abs/1902.07399)] [[Code](https://github.com/yrahul3910/adaptive-lr-dnn)]
- Bag of Freebies for Training Object Detection Neural Networks [[arXiv](https://arxiv.org/abs/1902.04103)]
- Parameter-Efficient Transfer Learning for NLP [[arXiv](https://arxiv.org/abs/1902.00751)]

#### 2019-01
- Multi-Task Deep Neural Networks for Natural Language Understanding [[arXiv](https://arxiv.org/abs/1901.11504)] [[Code](https://github.com/namisan/mt-dnn)]
- Learning from Dialogue after Deployment: Feed Yourself, Chatbot! [[arXiv](https://arxiv.org/abs/1901.05415)]
- Semi-Unsupervised Learning with Deep Generative Models: Clustering and Classifying using Ultra-Sparse Labels [[arXiv](https://arxiv.org/abs/1901.08560)] [[Code](https://github.com/SemiUnsupervisedLearning/DGMs_for_semi-unsupervised_learning)]
- Attentive Neural Processes [[arXiv](https://arxiv.org/abs/1901.05761)]
- Pay Less Attention with Lightweight and Dynamic Convolutions [[arXiv](https://arxiv.org/abs/1901.10430)]
- Cross-lingual Language Model Pretraining [[arXiv](https://arxiv.org/abs/1901.07291)] [[Code](https://github.com/facebookresearch/XLM)]
- The Evolved Transformer [[arXiv](https://arxiv.org/abs/1901.11117)]
- Glyce: Glyph-vectors for Chinese Character Representations [[arXiv](https://arxiv.org/abs/1901.10125)] 
- Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [[arXiv](https://arxiv.org/abs/1901.02860)] [[Code]()]
- Panoptic Feature Pyramid Networks [[arXiv](https://arxiv.org/abs/1901.02446)]

#### 2018-12
- A Tutorial on Deep Latent Variable Models of Natural Language [[arXiv](https://arxiv.org/abs/1812.06834)]
- On the Dimensionality of Word Embedding [[arXiv](https://arxiv.org/abs/1812.04224)] [[Code](https://github.com/ziyin-dl/word-embedding-dimensionality-selection)]
- Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling [[arXiv](https://arxiv.org/abs/1812.10860)] [[Code](https://github.com/jsalt18-sentence-repl/jiant)]
- The Design and Implementation of XiaoIce, an Empathetic Social Chatbot [[arXiv](https://arxiv.org/abs/1812.08989)]
- Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors [[arXiv](https://arxiv.org/abs/1812.08985)]
- SlowFast Networks for Video Recognition [[arXiv](https://arxiv.org/abs/1812.03982)]
- Bag of Tricks for Image Classification with Convolutional Neural Networks [[arXiv](https://arxiv.org/abs/1812.01187)] [[Code](https://github.com/dmlc/gluon-cv)]

#### ICLR-19
- RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space [[arXiv](https://arxiv.org/abs/1902.10197)] [[Code](https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding)]
- Differentiable Learning-to-Normalize via Switchable Normalization [[arXiv](https://arxiv.org/abs/1806.10779)] [[Code](https://github.com/switchablenorms/Switchable-Normalization)]
- Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset [[arXiv](https://arxiv.org/abs/1810.12247)] [[dataset](https://g.co/magenta/maestro-dataset)]
- ProMP: Proximal Meta-Policy Search [[arXiv](https://arxiv.org/abs/1810.06784)]
- Slimmable Neural Networks [[arXiv](https://arxiv.org/abs/1812.08928)] [[Code](https://github.com/JiahuiYu/slimmable_networks)]
- Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1810.09536)] [[Code](https://github.com/yikangshen/Ordered-Neurons)]
- ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA [[Code](https://github.com/xchen-tamu/alista)]
- KnockoffGAN: Generating Knockoffs for Feature Selection using Generative Adversarial Networks 
- Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations [[arXiv](https://arxiv.org/abs/1807.01697)] [[Code](https://github.com/hendrycks/robustness)]
- Rethinking the Value of Network Pruning [[arXiv](https://arxiv.org/abs/1810.05270)] [[Code](https://github.com/Eric-mingjie/rethinking-network-pruning)]
- Large Scale GAN Training for High Fidelity Natural Image Synthesis [[arXiv](https://arxiv.org/abs/1809.11096)]

#### 2018-11
- Differentiating Concepts and Instances for Knowledge Graph Embedding [[arXiv](https://arxiv.org/abs/1811.04588)]  [[Code](https://github.com/davidlvxin/TransC)]
- Gradient Harmonized Single-stage Detector [[arXiv](https://arxiv.org/abs/1811.05181)] [[Code](https://github.com/libuyu/GHM_Detection)]
- Guiding Policies with Language via Meta-Learning [[arXiv](https://arxiv.org/abs/1811.07882)]
- Dataset Distillation [[arXiv](https://arxiv.org/abs/1811.10959)]
- GAN Dissection: Visualizing and Understanding Generative Adversarial Networks [[arXiv](https://arxiv.org/abs/1811.10597)] [[Code](https://github.com/CSAILVision/gandissect)]
- Towards Explainable NLP: A Generative Explanation Framework for Text Classification [[arXiv](https://arxiv.org/abs/1811.00196)]
- Rethinking ImageNet Pre-training [[arXiv](https://arxiv.org/abs/1811.08883)]
- Sampling Can Be Faster Than Optimization [[arXiv](https://arxiv.org/abs/1811.08413)]
- Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms? [[arXiv](https://arxiv.org/abs/1811.02553)]
- Gradient Descent Finds Global Minima of Deep Neural Networks [[arXiv](https://arxiv.org/abs/1811.03804)]

#### 2018-10
- Three Mechanisms of Weight Decay Regularization [[arXiv](https://arxiv.org/abs/1810.12281)]
- Neural Nearest Neighbors Networks [[arXiv](https://arxiv.org/abs/1810.12575)] [[Code](https://github.com/visinf/n3net/)]
- Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow [[arXiv](https://arxiv.org/abs/1810.00821)] [[Code](https://github.com/akanazawa/vgan)]
- DropBlock: A regularization method for convolutional networks [[arXiv](https://arxiv.org/abs/1810.12890)]
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [[arXiv](https://arxiv.org/abs/1810.04805)][[Code](https://github.com/google-research/bert)][[Code](https://github.com/codertimo/BERT-pytorch)]
- Gradient Descent Provably Optimizes Over-parameterized Neural Networks [[arXiv](https://arxiv.org/abs/1810.02054)]

#### 2018-09
- Deep Interest Evolution Network for Click-Through Rate Prediction [[arXiv](https://arxiv.org/abs/1809.03672)]
- Multi-Source Domain Adaptation with Mixture of Experts [[arXiv](https://arxiv.org/abs/1809.02256)][[Code](https://github.com/jiangfeng1124/transfer)]
- Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation [[arXiv](https://arxiv.org/abs/1809.02094)] [[Code](https://github.com/artetxem/uncovec)]
- Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding [[arXiv](https://arxiv.org/abs/1809.03702)]

#### NIPS-18
- Optimal Algorithms for Non-Smooth Distributed Optimization in Networks [[arXiv](https://arxiv.org/abs/1806.00291)]
- Non-delusional Q-learning and value-iteration
- Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes
- Neural Ordinary Differential Equations [[arXiv](https://arxiv.org/abs/1806.07366)]
- How Does Batch Normalization Help Optimization? [[arXiv](https://arxiv.org/abs/1805.11604)]
- Learning Disentangled Joint Continuous and Discrete Representations [[arXiv](https://arxiv.org/abs/1804.00104)] [[Code](https://github.com/Schlumberger/joint-vae)]
- Tree-to-tree Neural Networks for Program Translation [[arXiv](https://arxiv.org/abs/1802.03691)]
- Distilled Wasserstein Learning for Word Embedding and Topic Modeling [[arXiv](https://arxiv.org/abs/1809.04705)]
- Reversible Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1810.10999)][[Code](https://github.com/matthewjmackay/reversible-rnn)]
- Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding [[arXiv](https://arxiv.org/abs/1810.02338)]

#### 2018-08
- Understanding Back-Translation at Scale [[arXiv](https://arxiv.org/abs/1808.09381)] [[Code](https://github.com/pytorch/fairseq)]
- Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures [[arXiv](https://arxiv.org/abs/1808.08946)]
- Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction [[arXiv](https://arxiv.org/abs/1808.03867)] [[Code](https://github.com/elbayadm/attn2d)]
- Learning Neural Templates for Text Generation [[arXiv](https://arxiv.org/abs/1808.10122)] [[Code](https://github.com/harvardnlp/neural-template-gen)]
- Is Wasserstein all you need? [[arXiv](https://arxiv.org/abs/1808.09663)]
- Training Deeper Neural Machine Translation Models with Transparent Attention [[arXiv](https://arxiv.org/abs/1808.07561)]
- Contextual Parameter Generation for Universal Neural Machine Translation [[arXiv](https://arxiv.org/abs/1808.08493)][[Code](https://github.com/eaplatanios/symphony-mt)]
- CoQA: A Conversational Question Answering Challenge [[arXiv](https://arxiv.org/abs/1808.07042)] [[dataset](https://stanfordnlp.github.io/coqa/)]
- Learning deep representations by mutual information estimation and maximization [[arXiv](https://arxiv.org/abs/1808.06670)]
- Large-Scale Study of Curiosity-Driven Learning [[arXiv](https://arxiv.org/abs/1808.04355)] [[Code](https://github.com/openai/large-scale-curiosity)]
- A Study of Reinforcement Learning for Neural Machine Translation [[arXiv](https://arxiv.org/abs/1808.08866)] [[Code](https://github.com/apeterswu/RL4NMT)]

#### 2018-07
- Implementing Neural Turing Machines [[arXiv](https://arxiv.org/abs/1807.08518)] [[Code](https://github.com/MarkPKCollier/NeuralTuringMachine)]
- Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer [[arXiv](https://arxiv.org/abs/1807.07543)] [[Code](https://github.com/brain-research/acai)] [[Code](https://gist.github.com/kylemcdonald/e8ca989584b3b0e6526c0a737ed412f0)]
- Glow: Generative Flow with Invertible 1x1 Convolutions [[arXiv](https://arxiv.org/abs/1807.03039)] [[Code](https://github.com/openai/glow)]
- Representation Learning with Contrastive Predictive Coding [[arXiv](https://arxiv.org/abs/1807.03748)]
- Latent Alignment and Variational Attention [[arXiv](https://arxiv.org/abs/1807.03756)] [[Code](https://github.com/harvardnlp/var-attn/)]
- The GAN Landscape: Losses, Architectures, Regularization, and Normalization [[arXiv](https://arxiv.org/abs/1807.04720)] [[Code](https://github.com/google/compare_gan)]
- Conditional Neural Processes [[arXiv](https://arxiv.org/abs/1807.01613)]
- Neural Processes [[arXiv](https://arxiv.org/abs/1807.01622)]
- Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study [[arXiv](https://arxiv.org/abs/1807.01270)]

#### 2018-06
- Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series [[arXiv](https://arxiv.org/abs/1806.02199)] [[Code](https://github.com/ratschlab/SOM-VAE)]
- DARTS: Differentiable Architecture Search [[arXiv](https://arxiv.org/abs/1806.09055)] [[Code](https://github.com/quark0/darts)]
- Auto-Keras: Efficient Neural Architecture Search with Network Morphism [[arXiv](https://arxiv.org/abs/1806.10282)] [[Code](https://autokeras.com/)]
- Design Challenges and Misconceptions in Neural Sequence Labeling [[arXiv](https://arxiv.org/abs/1806.04470)] [[Code](https://github.com/jiesutd/NCRFpp)]
- Guided evolutionary strategies: escaping the curse of dimensionality in random search [[arXiv](https://arxiv.org/abs/1806.10230)]
- Hierarchical Graph Representation Learning with Differentiable Pooling [[arXiv](https://arxiv.org/abs/1806.08804)]
- Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering [[arXiv](https://arxiv.org/abs/1806.04330)] [[Code](https://github.com/lanwuwei/SPM_toolkit)]
- Gaussian mixture models with Wasserstein distance [[arXiv](https://arxiv.org/abs/1806.04465)]
- Relational recurrent neural networks [[arXiv](https://arxiv.org/abs/1806.01822)]
- RUDDER: Return Decomposition for Delayed Rewards [[arXiv](https://arxiv.org/abs/1806.07857)] [[Code](https://github.com/ml-jku/baselines-rudder)]
- GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations [[arXiv](https://arxiv.org/abs/1806.05662)]
- Know What You Don't Know: Unanswerable Questions for SQuAD [[arXiv](https://arxiv.org/abs/1806.03822)] [[dataset](https://rajpurkar.github.io/SQuAD-explorer/)]
- Relational inductive biases, deep learning, and graph networks [[arXiv](https://arxiv.org/abs/1806.01261)] [[Code](https://github.com/deepmind/graph_nets)]

#### ICML-18
- Fast Decoding in Sequence Models using Discrete Latent Variables [[arXiv](https://arxiv.org/abs/1803.03382)]
- Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples [[arXiv](https://arxiv.org/abs/1802.00420)] [[Code](https://github.com/anishathalye/obfuscated-gradients)]
- Delayed Impact of Fair Machine Learning [[arXiv](https://arxiv.org/abs/1803.04383)]

#### 2018-05
- Chinese NER Using Lattice LSTM [[arXiv](https://arxiv.org/abs/1805.02023)] [[Code](https://github.com/jiesutd/LatticeLSTM)]
- Transformation Networks for Target-Oriented Sentiment Classification [[arXiv](https://arxiv.org/abs/1805.01086)] [[Code](https://github.com/lixin4ever/TNet)]
- Hybrid semi-Markov CRF for Neural Sequence Labeling [[arXiv](https://arxiv.org/abs/1805.03838)] [[Code](https://github.com/ZhixiuYe/HSCRF-pytorch)]
- What you can cram into a single vector: Probing sentence embeddings for linguistic properties [[arXiv](https://arxiv.org/abs/1805.01070)] [[Code](https://github.com/facebookresearch/SentEval/tree/master/data/probing)]
- Training Classifiers with Natural Language Explanations [[arXiv](https://arxiv.org/abs/1805.03818)] [[Code](https://github.com/HazyResearch/babble)]
- Deep Reinforcement Learning For Sequence to Sequence Models [[arXiv](https://arxiv.org/abs/1805.09461)] [[Code](https://github.com/yaserkl/RLSeq2Seq)]
- Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms [[arXiv](https://arxiv.org/abs/1805.09843)] [[Code](https://github.com/dinghanshen/SWEM)]
- Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information [[arXiv](https://arxiv.org/abs/1805.04655)] [[Code](https://github.com/raosudha89/ranking_clarification_questions)]
- Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context [[arXiv](https://arxiv.org/abs/1805.04623)] [[Code](https://github.com/urvashik/lm-context-analysis)]
- AutoAugment: Learning Augmentation Policies from Data [[arXiv](https://arxiv.org/abs/1805.09501)]
- Meta-Gradient Reinforcement Learning [[arXiv](https://arxiv.org/abs/1805.09801)]
- Born Again Neural Networks [[arXiv](https://arxiv.org/abs/1805.04770)]
- Convolutional CRFs for Semantic Segmentation [[arXiv](https://arxiv.org/abs/1805.04777)]  [[Code](https://github.com/MarvinTeichmann/ConvCRF)]
- Did the Model Understand the Question? [[arXiv](https://arxiv.org/abs/1805.05492)] [[Code](https://github.com/pramodkaushik/acl18_results)]
- From Word to Sense Embeddings: A Survey on Vector Representations of Meaning [[arXiv](https://arxiv.org/abs/1805.04032)]
- Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review [[arXiv](https://arxiv.org/abs/1805.00909)]

#### 2018-04
- The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation [[arXiv](https://arxiv.org/abs/1804.09849)]
- Linguistically-Informed Self-Attention for Semantic Role Labeling [[arXiv](https://arxiv.org/abs/1804.08199)]
- Taskonomy: Disentangling Task Transfer Learning [[arXiv](https://arxiv.org/abs/1804.08328)] [[Code](https://github.com/StanfordVL/taskonomy)]
- Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation [[arXiv](https://arxiv.org/abs/1804.08069)] [[Code](https://github.com/snakeztc/NeuralDialog-LAED)]
- Learning Semantic Textual Similarity from Conversations [[arXiv](https://arxiv.org/abs/1804.07754)]
- When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation? [[arXiv](https://arxiv.org/abs/1804.06323)]
- QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension [[arXiv](https://arxiv.org/abs/1804.09541)] [[Code](https://github.com/hengruo/QANet-pytorch)]
- Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models [[arXiv](https://arxiv.org/abs/1804.09299)] [[Code](https://github.com/HendrikStrobelt/Seq2Seq-Vis)]
- Low Rank Structure of Learned Representations [[arXiv](https://arxiv.org/abs/1804.07090)]
- Decoupled Networks [[arXiv](https://arxiv.org/abs/1804.08071)]
- Learned Deformation Stability in Convolutional Neural Networks [[arXiv](https://arxiv.org/abs/1804.04438)]
- Phrase-Based & Neural Unsupervised Machine Translation [[arXiv](https://arxiv.org/abs/1804.07755)]
- Learning to Map Context-Dependent Sentences to Executable Formal Queries [[arXiv](https://arxiv.org/abs/1804.06868)] [[Code](https://github.com/clic-lab/atis)]
- Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations [[arXiv](https://arxiv.org/abs/1804.02485)]
- Differentiable plasticity: training plastic neural networks with backpropagation [[arXiv](https://arxiv.org/abs/1804.02464)]
- Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning [[arXiv](https://arxiv.org/abs/1804.00079)] [[Code](https://github.com/Maluuba/gensen)]

#### 2018-03
- An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling [[arXiv](https://arxiv.org/abs/1803.01271)] [[Code](https://github.com/locuslab/TCN)]
- Variance Networks: When Expectation Does Not Meet Your Expectations [[arXiv](https://arxiv.org/abs/1803.03764)]
- Referring Relationships [[arXiv](https://arxiv.org/abs/1803.10362)]
- Iterative Visual Reasoning Beyond Convolutions [[arXiv](https://arxiv.org/abs/1803.11189)]
- World Models [[arXiv](https://arxiv.org/abs/1803.10122)]
- Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning [[arXiv](https://arxiv.org/abs/1803.05268)]
- A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay [[arXiv](https://arxiv.org/abs/1803.09820)]
- Feudal Reinforcement Learning for Dialogue Management in Large Domains [[arXiv](https://arxiv.org/abs/1803.03232)]
- Universal Sentence Encoder [[arXiv](https://arxiv.org/abs/1803.11175)]
- Averaging Weights Leads to Wider Optima and Better Generalization [[arXiv](https://arxiv.org/abs/1803.05407)]
- On the importance of single directions for generalization [[arXiv](https://arxiv.org/abs/1803.06959)]
- Group Normalization [[arXiv](https://arxiv.org/abs/1803.08494)]
- Compositional Attention Networks for Machine Reasoning [[arXiv](https://arxiv.org/abs/1803.03067)]
- Learning Longer-term Dependencies in RNNs with Auxiliary Losses [[arXiv](https://arxiv.org/abs/1803.00144)]

#### 2018-02
- Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and Reviews [[arXiv](https://arxiv.org/abs/1802.07938)] [[Code](https://github.com/guoyang9/A3NCF)]
- Regularized Evolution for Image Classifier Architecture Search [[arXiv](https://arxiv.org/abs/1802.01548)]
- Visual Interpretability for Deep Learning: a Survey [[arXiv](https://arxiv.org/abs/1802.00614)]
- Deep contextualized word representation [[arXiv](https://arxiv.org/abs/1802.05365)]
- Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs [[arXiv](https://arxiv.org/abs/1802.10026)] [[Code](https://github.com/timgaripov/dnn-mode-connectivity)]
- Machine Theory of Mind [[arXiv](https://arxiv.org/abs/1802.07740)] 
- Efficient Neural Architecture Search via Parameter Sharing [[arXiv](https://arxiv.org/abs/1802.03268)]
- Interpreting CNNs via Decision Trees [[arXiv](https://arxiv.org/abs/1802.00121)]
- DensePose: Dense Human Pose Estimation In The Wild [[arXiv](https://arxiv.org/abs/1802.00434)] [[dataset](http://densepose.org/)]
- DeepType: Multilingual Entity Linking by Neural Type System Evolution [[arXiv](https://arxiv.org/abs/1802.01021)] [[Code](https://github.com/openai/deeptype)]
- Recent Advances in Neural Program Synthesis [[arXiv](https://arxiv.org/abs/1802.02353)]
- IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [[arXiv](https://arxiv.org/abs/1802.01561)] [[Code](https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/dmlab30)]
- Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples [[arXiv](https://arxiv.org/abs/1802.00420)] [[Code](https://github.com/anishathalye/obfuscated-gradients)]

#### 2018-01
- Universal Language Model Fine-tuning for Text Classification [[arXiv](https://arxiv.org/abs/1801.06146)]
- Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling [[arXiv](https://arxiv.org/abs/1801.10296)]
- Quantum Computing in the NISQ era and beyond [[arXiv](https://arxiv.org/abs/1801.00862)]
- Active Neural Localization [[arXiv](https://arxiv.org/abs/1801.08214)]
- Global overview of Imitation Learning [[arXiv](https://arxiv.org/abs/1801.06503)]
- MaskGAN: Better Text Generation via Filling in the ______ [[arXiv](https://arxiv.org/abs/1801.07736)] [[Code](https://github.com/tensorflow/models/tree/master/research/maskgan)]
- Deep Learning for Sentiment Analysis : A Survey [[arXiv](https://arxiv.org/abs/1801.07883)]
- Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift [[arXiv](https://arxiv.org/abs/1801.05134)]
- DENSER: Deep Evolutionary Network Structured Representation [[arXiv](https://arxiv.org/abs/1801.01563)]
- Building a Conversational Agent Overnight with Dialogue Self-Play [[arXiv](https://arxiv.org/abs/1801.04871)]
- Unsupervised Real-to-Virtual Domain Unification for End-to-End Highway Driving [[arXiv](https://arxiv.org/abs/1801.03458)]
- SBNet: Sparse Blocks Network for Fast Inference [[arXiv](https://arxiv.org/abs/1801.02108)]
- Deep Stereo Matching with Explicit Cost Aggregation Sub-Architecture [[arXiv](https://arxiv.org/abs/1801.04065)]
- Adversarial Spheres [[arXiv](https://arxiv.org/abs/1801.02774)]
- Neural Program Synthesis with Priority Queue Training [[arXiv](https://arxiv.org/abs/1801.03526)] [[Code](https://github.com/tensorflow/models/tree/master/research/brain_coder)]
- Adversarial Generative Nets: Neural Network Attacks on State-of-the-Art Face Recognition [[arXiv](https://arxiv.org/abs/1801.00349)]
- DeepMind Control Suite [[arXiv](https://arxiv.org/abs/1801.00690)] [[dataset](https://github.com/deepmind/dm_control)]

#### 2017-12
- Noisy Natural Gradient as Variational Inference [[arXiv](https://arxiv.org/abs/1712.02390)] [[Code](https://github.com/wlwkgus/NoisyNaturalGradient)]
- Non-convex Optimization for Machine Learning [[arXiv](https://arxiv.org/abs/1712.07897)]
- Improving Generalization Performance by Switching from Adam to SGD [[arXiv](https://arxiv.org/abs/1712.07628)]
- Learning by Asking Questions [[arXiv](https://arxiv.org/abs/1712.01238)]
- A Flexible Approach to Automated RNN Architecture Generation [[arXiv](https://arxiv.org/abs/1712.07316)]
- Lectures on Randomized Numerical Linear Algebra [[arXiv](https://arxiv.org/abs/1712.08880)]
- Data Distillation: Towards Omni-Supervised Learning [[arXiv](https://arxiv.org/abs/1712.04440)]
- Deep Learning Scaling is Predictable, Empirically [[arXiv](https://arxiv.org/abs/1712.00409)]
- SGAN: An Alternative Training of Generative Adversarial Networks [[arXiv](https://arxiv.org/abs/1712.02330)]
- The NarrativeQA Reading Comprehension Challenge [[arXiv](https://arxiv.org/abs/1712.07040)] [[dataset](https://github.com/deepmind/narrativeqa)]
- Regularization and Optimization strategies in Deep Convolutional Neural Network [[arXiv](https://arxiv.org/abs/1712.04711)]
- Mathematics of Deep Learning [[arXiv](https://arxiv.org/abs/1712.04741)]
- Text Generation Based on Generative Adversarial Nets with Latent Variable [[arXiv](https://arxiv.org/abs/1712.00170)]
- Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
- Show-and-Fool: Crafting Adversarial Examples for Neural Image Captioning [[arXiv](https://arxiv.org/abs/1712.02051)] [[Code](https://github.com/huanzhang12/ImageCaptioningAttack)]

#### ICLR-18
- Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling
- i-RevNet: Deep Invertible Networks [[arXiv](https://arxiv.org/abs/1802.07088)] [[Code](https://github.com/jhjacobsen/pytorch-i-revnet)]
- Hierarchical Representations for Efficient Architecture Search [[arXiv](https://arxiv.org/abs/1711.00436)]
- Backpropagation through the Void: Optimizing control variates for black-box gradient estimation [[arXiv](https://arxiv.org/abs/1711.00123)] [[Code](https://github.com/duvenaud/relax)]
- Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection
- Wasserstein Auto-Encoders [[arXiv](https://arxiv.org/abs/1711.01558)]
- Neural Speed Reading via Skim-RNN [[arXiv](https://arxiv.org/abs/1711.02085)]
- Breaking the Softmax Bottleneck: A High-Rank RNN Language Model [[arXiv](https://arxiv.org/abs/1711.03953)] [[Code](https://github.com/zihangdai/mos)]
- Non-Autoregressive Neural Machine Translation [[arXiv](https://arxiv.org/abs/1711.02281)]
- A DIRT-T Approach to Unsupervised Domain Adaptation
- On the Information Bottleneck Theory of Deep Learning
- mixup: Beyond Empirical Risk Minimization
- Unsupervised Machine Translation Using Monolingual Corpora Only [[arXiv](https://arxiv.org/abs/1711.00043)]
- Matrix capsules with EM routing

#### 2017-11
- Hierarchical Representations for Efficient Architecture Search [[arXiv](https://arxiv.org/abs/1711.00436)]
- Embedding Words as Distributions with a Bayesian Skip-gram Model [[arXiv](https://arxiv.org/abs/1711.11027)]
- Deep Image Prior [[arXiv](https://arxiv.org/abs/1711.10925)] [[Code](https://github.com/DmitryUlyanov/deep-image-prior)]
- MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1711.06788)]
- Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning [[arXiv](https://arxiv.org/abs/1711.07613)]
- Neural Text Generation: A Practical Guide [[arXiv](https://arxiv.org/abs/1711.09534)]
- Memory Aware Synapses: Learning what (not) to forget [[arXiv](https://arxiv.org/abs/1711.09601)]
- Are GANs Created Equal? A Large-Scale Study [[arXiv](https://arxiv.org/abs/1711.10337)]
- Distilling a Neural Network Into a Soft Decision Tree [[arXiv](https://arxiv.org/abs/1711.09784)]
- SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability [[arXiv](https://arxiv.org/abs/1706.05806)] [[Code](https://github.com/google/svcca)]
- Non-local Neural Networks [[arXiv](https://arxiv.org/abs/1711.07971)]
- Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations [[arXiv](https://arxiv.org/abs/1711.05732)] [[dataset](https://github.com/jwieting)] [[Code](https://github.com/jwieting)]
- Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks [[arXiv](https://arxiv.org/abs/1711.00350)] [[dataset](https://github.com/brendenlake/SCAN)]
- Neural Discrete Representation Learning [[arXiv](https://arxiv.org/abs/1711.00937)]
- Weighted Transformer Network for Machine Translation [[arXiv](https://arxiv.org/abs/1711.02132)]

#### 2017-10
- Dynamic Routing Between Capsules [[arXiv](https://arxiv.org/abs/1710.09829)]
- Unsupervised Neural Machine Translation [[arXiv](https://arxiv.org/abs/1710.11041)]

#### 2017-09
- Empower Sequence Labeling with Task-Aware Neural Language Model [[arXiv](https://arxiv.org/abs/1709.04109)] [[Code](https://github.com/LiyuanLucasLiu/LM-LSTM-CRF)]
- Dynamic Evaluation of Neural Sequence Models [[arXiv](https://arxiv.org/abs/1709.07432)] [[Code](https://github.com/benkrause/dynamic-evaluation)]

#### NIPS-17
- Improved Training of Wasserstein GANs [[arXiv](https://arxiv.org/abs/1704.00028)] [[Code](https://github.com/igul222/improved_wgan_training)] [[Code](https://github.com/caogang/wgan-gp)]
- The Reversible Residual Network: Backpropagation Without Storing Activations [[arXiv](https://arxiv.org/abs/1707.04585)] [[Code](https://github.com/renmengye/revnet-public)] [[Code](https://github.com/tbung/pytorch-revnet)]
- Plan, Attend, Generate: Planning for Sequence-to-Sequence Models [[arXiv](https://arxiv.org/abs/1711.10462)]
- REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models [[arXiv](https://arxiv.org/abs/1703.07370)] [[Code](https://github.com/tensorflow/models/tree/master/research/rebar)]
- Poincaré Embeddings for Learning Hierarchical Representations [[arXiv](https://arxiv.org/abs/1705.08039)]
- Character-Level Language Modeling with Recurrent Highway Hypernetworks [[Code](https://github.com/jsuarez5341/Recurrent-Highway-Hypernetworks-NIPS)] 
- Dilated Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1710.02224)] [[Code](https://github.com/code-terminator/DilatedRNN)]
- Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning [[arXiv](https://arxiv.org/abs/1711.01577)]
- Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations [[arXiv](https://arxiv.org/abs/1704.00648)]
- Learned in Translation: Contextualized Word Vectors [[arXiv](https://arxiv.org/abs/1708.00107)] [[Code](https://github.com/salesforce/cove)]

#### 2017-08
- Neural Collaborative Filtering [[arXiv](https://arxiv.org/abs/1708.05031)] [[Code](https://github.com/guoyang9/NCF)]
- Focal Loss for Dense Object Detection [[arXiv](https://arxiv.org/abs/1708.02002)]
- Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge [[arXiv](https://arxiv.org/abs/1708.02711)] [[Code](https://github.com/hengyuan-hu/bottom-up-attention-vqa)]

#### 2017-07
- Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering [[arXiv](https://arxiv.org/abs/1707.07998)] [[Code](https://github.com/hengyuan-hu/bottom-up-attention-vqa)]

#### 2017-06
- Deep Interest Network for Click-Through Rate Prediction [[arXiv](https://arxiv.org/abs/1706.06978)]
- Self-Normalizing Neural Networks [[arXiv](https://arxiv.org/abs/1706.02515)]
- One Model To Learn Them All [[arXiv](https://arxiv.org/abs/1706.05137)] [[Code](https://github.com/tensorflow/tensor2tensor)]
- Attention Is All You Need [[arXiv](https://arxiv.org/abs/1706.03762)] [[Code](https://github.com/tensorflow/tensor2tensor)]
- A simple neural network module for relational reasoning [[arXiv](https://arxiv.org/abs/1706.01427)]

#### ICML-17
- Large-Scale Evolution of Image Classifiers [[arXiv](https://arxiv.org/abs/1703.01041)]
- Improved Variational Autoencoders for Text Modeling using Dilated Convolutions [[arXiv](https://arxiv.org/abs/1702.08139)]
- Conditional Image Synthesis With Auxiliary Classifier GANs [[arXiv](https://arxiv.org/abs/1610.09585)]
- Toward Controlled Generation of Text [[arXiv](https://arxiv.org/abs/1703.00955)]

#### 2017-05
- Ask the Right Questions: Active Question Reformulation with Reinforcement Learning [[arXiv](https://arxiv.org/abs/1705.07830)]
- Supervised Learning of Universal Sentence Representations from Natural Language Inference Data [[arXiv](https://arxiv.org/abs/1705.02364)] [[Code](https://github.com/facebookresearch/InferSent)] [[Code](https://github.com/facebookresearch/SentEval)]
- Learning to Ask: Neural Question Generation for Reading Comprehension [[arXiv](https://arxiv.org/abs/1705.00106)] [[Code](https://github.com/xinyadu/nqg)]
- Adversarial Ranking for Language Generation [[arXiv](https://arxiv.org/abs/1705.11001)]
- Learning Structured Text Representations [[arXiv](https://arxiv.org/abs/1705.09207)] [[Code](https://github.com/nlpyang/structured)]
- TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension [[arXiv](https://arxiv.org/abs/1705.03551)] [[dataset](http://nlp.cs.washington.edu/triviaqa/)] [[Code](https://github.com/mandarjoshi90/triviaqa)]
- ParlAI: A Dialog Research Software Platform [[arXiv](https://arxiv.org/abs/1705.06476)] [[Code](https://github.com/facebookresearch/ParlAI)]

#### 2017-04
- Stochastic Gradient Descent as Approximate Bayesian Inference [[arXiv](https://arxiv.org/abs/1704.04289)]
- Snapshot Ensembles: Train 1, get M for free [[arXiv](https://arxiv.org/abs/1704.00109)] [[Code](https://github.com/gaohuang/SnapshotEnsemble)] [[Code](https://github.com/titu1994/Snapshot-Ensembles)]
- From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood [[arXiv](https://arxiv.org/abs/1704.07926)] [[Code](https://github.com/kelvinguu/lang2program)]
- Reading Wikipedia to Answer Open-Domain Questions [[arXiv](https://arxiv.org/abs/1704.00051)] [[Code](https://github.com/facebookresearch/DrQA)]
- Learning to Skim Text [[arXiv](https://arxiv.org/abs/1704.06877)]

#### 2017-03
- Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [[arXiv](https://arxiv.org/abs/1703.10593)] [[Code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)]
- Evolution Strategies as a Scalable Alternative to Reinforcement Learning [[arXiv](https://arxiv.org/abs/1703.03864)] [[Code](https://github.com/openai/evolution-strategies-starter)] [[Code](https://github.com/atgambardella/pytorch-es)]

#### 2017-02
- Exploring loss function topology with cyclical learning rates [[arXiv](https://arxiv.org/abs/1702.04283)] [[Code](https://github.com/lnsmith54/exploring-loss)]
- A Hybrid Convolutional Variational Autoencoder for Text Generation [[arXiv](https://arxiv.org/abs/1702.02390)] [[Code](https://github.com/stas-semeniuta/textvae)]

#### 2017-01
- Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer [[arXiv](https://arxiv.org/abs/1701.06538)] [[Code](https://github.com/davidmrau/mixture-of-experts)]
- Wasserstein GAN [[arXiv](https://arxiv.org/abs/1701.07875)] [[Code](https://github.com/martinarjovsky/WassersteinGAN)]
- Adversarial Learning for Neural Dialogue Generation [[arXiv](https://arxiv.org/abs/1701.06547)]
- Deep Reinforcement Learning: An Overview [[arXiv](https://arxiv.org/abs/1701.07274)]
- OpenNMT: Open-Source Toolkit for Neural Machine Translation [[arXiv](https://arxiv.org/abs/1701.02810)] [[Code](https://github.com/OpenNMT/OpenNMT)] [[Code](https://github.com/OpenNMT/OpenNMT-py)]

#### ICLR-17
- Adversarial Training Methods for Semi-Supervised Text Classification [[arXiv](https://arxiv.org/abs/1605.07725)] [[Code](https://github.com/tensorflow/models/tree/master/research/adversarial_text)]
- Structured Attention Networks [[arXiv](https://arxiv.org/abs/1702.00887)] [[Code](https://github.com/harvardnlp/struct-attn)]
- Learning End-to-End Goal-Oriented Dialog [[arXiv](https://arxiv.org/abs/1605.07683)] [[dataset](http://fb.ai/babi)]
- Understanding deep learning requires rethinking generalization [[arXiv](https://arxiv.org/abs/1611.03530)]
- An Actor-Critic Algorithm for Sequence Prediction [[arXiv](https://arxiv.org/abs/1607.07086)]
- Learning to Remember Rare Events [[arXiv](https://arxiv.org/abs/1703.03129)]
- Introspection: Accelerating Neural Network Training By Learning Weight Evolution [[arXiv](https://arxiv.org/abs/1704.04959)]

#### 2016-11
- Categorical Reparameterization with Gumbel-Softmax [[arXiv](https://arxiv.org/abs/1611.01144)]
- A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks [[arXiv](https://arxiv.org/abs/1611.01587)]
- Image-to-Image Translation with Conditional Adversarial Networks [[arXiv](https://arxiv.org/abs/1611.07004)] [[Code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)]

#### 2016-10
- Using Fast Weights to Attend to the Recent Past [[arXiv](https://arxiv.org/abs/1610.06258)]

#### 2016-09
- HyperNetworks [[arXiv](https://arxiv.org/abs/1609.09106)]
- Language as a Latent Variable: Discrete Generative Models for Sentence Compression [[arXiv](https://arxiv.org/abs/1609.07317)]
- SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient [[arXiv](https://arxiv.org/abs/1609.05473)] [[Code](https://github.com/LantaoYu/SeqGAN)]

#### 2016-08
- Densely Connected Convolutional Networks [[arXiv](https://arxiv.org/abs/1608.06993)] [[Code](https://github.com/liuzhuang13/DenseNet)]
- SGDR: Stochastic Gradient Descent with Warm Restarts [[arXiv](https://arxiv.org/abs/1608.03983)] [[Code](https://github.com/loshchil/SGDR)]

#### 2016-07
- Enriching Word Vectors with Subword Information [[arXiv](https://arxiv.org/abs/1607.04606)] [[Code](https://github.com/facebookresearch/fastText)]
- Bag of Tricks for Efficient Text Classification [[arXiv](https://arxiv.org/abs/1607.01759)] [[Code](https://github.com/facebookresearch/fastText)]

#### 2016-06
- Gaussian Error Linear Units (GELUs) [[arXiv](https://arxiv.org/abs/1606.08415)] 
- A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task [[arXiv](https://arxiv.org/abs/1606.02858)]
- Convolution by Evolution: Differentiable Pattern Producing Networks [[arXiv](https://arxiv.org/abs/1606.02580)]
- Conditional Image Generation with PixelCNN Decoders [[arXiv](https://arxiv.org/abs/1606.05328)]
- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets [[arXiv](https://arxiv.org/abs/1606.03657)]

#### 2016-04
- Benchmarking Deep Reinforcement Learning for Continuous Control [[arXiv](https://arxiv.org/abs/1604.06778)] [[Code](https://github.com/rll/rllab)]

#### 2016-03
- XGBoost: A Scalable Tree Boosting System [[arXiv](https://arxiv.org/abs/1603.02754)]
- Text Understanding with the Attention Sum Reader Network [[arXiv](https://arxiv.org/abs/1603.01547)]

#### 2016-02
- Associative Long Short-Term Memory [[arXiv](https://arxiv.org/abs/1602.03032)]

#### 2015-11
- Neural Variational Inference for Text Processing [[arXiv](https://arxiv.org/abs/1511.06038)]
- Generating Sentences from a Continuous Space [[arXiv](https://arxiv.org/abs/1511.06349)]

#### 2015-06
- Cyclical Learning Rates for Training Neural Networks [[arXiv](https://arxiv.org/abs/1506.01186)] [[Code](https://github.com/bckenstler/CLR)]
- Skip-Thought Vectors [[arXiv](https://arxiv.org/abs/1506.06726)]

#### 2015-05
- U-Net: Convolutional Networks for Biomedical Image Segmentation [[arXiv](https://arxiv.org/abs/1505.04597)]

#### 2015-02
- Trust Region Policy Optimization [[arXiv](https://arxiv.org/abs/1502.05477)]

#### 2014-12
- Adam: A Method for Stochastic Optimization [[arXiv](https://arxiv.org/abs/1412.6980)]

#### 2014-11
- Conditional Generative Adversarial Nets [[arXiv](https://arxiv.org/abs/1411.1784)]

#### 2014-08
- Convolutional Neural Networks for Sentence Classification [[arXiv](https://arxiv.org/abs/1408.5882)]

#### 2014-06
- Generative Adversarial Networks [[arXiv](https://arxiv.org/abs/1406.2661)]
- Semi-Supervised Learning with Deep Generative Models [[arXiv](https://arxiv.org/abs/1406.5298)] [[Code](https://github.com/dpkingma/nips14-ssl)]

#### 2014-03
- DeepWalk: Online Learning of Social Representations [[arXiv](https://arxiv.org/abs/1403.6652)]

#### 2013-12
- Auto-Encoding Variational Bayes [[arXiv](https://arxiv.org/abs/1312.6114)]

#### 2012-06
- Variational Bayesian Inference with Stochastic Search [[arXiv](https://arxiv.org/abs/1206.6430)]
