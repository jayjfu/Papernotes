# papernotes
<!-- 2019-04， 郑哲东， CVPR2019（Oral） -->
#### 2019-11
- Momentum Contrast for Unsupervised Visual Representation Learning [[arXiv](https://arxiv.org/abs/1911.05722)]
- Real-Time Reinforcement Learning [[arXiv](https://arxiv.org/abs/1911.04448)]
- Self-training with Noisy Student improves ImageNet classification [[arXiv](https://arxiv.org/abs/1911.04252)]

#### 2019-10
- Contrastive Representation Distillation [[arXiv](https://arxiv.org/abs/1910.10699)] [[Code](https://github.com/HobbitLong/RepDistiller)]
- BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search [[arXiv](https://arxiv.org/abs/1910.11858)]
- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [[arXiv](https://arxiv.org/abs/1910.10683)] [[Code](https://github.com/google-research/text-to-text-transfer-transformer)]
- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter [[arXiv](https://arxiv.org/abs/1910.01108)]
- Generalized Inner Loop Meta-Learning [[arXiv](https://arxiv.org/abs/1910.01727)] [[Code](https://github.com/facebookresearch/higher)]

#### 2019-09
- Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT [[arXiv](https://arxiv.org/abs/1909.05840)]
- Meta-Learning with Implicit Gradients [[arXiv](https://arxiv.org/abs/1909.04630)]
- Unsupervised Domain Adaptation through Self-Supervision [[arXiv](https://arxiv.org/abs/1909.11825)] [[Code](https://github.com/yueatsprograms/uda_release)]
- Wider Networks Learn Better Features [[arXiv](https://arxiv.org/abs/1909.11572)]
- Gradient Descent: The Ultimate Optimizer [[arXiv](https://arxiv.org/abs/1909.13371)]

#### 2019-08
- One Model To Rule Them All [[arXiv](https://arxiv.org/abs/1908.03015)]
- AutoML: A Survey of the State-of-the-Art [[arXiv](https://arxiv.org/abs/1908.00709)]
- SenseBERT: Driving Some Sense into BERT [[arXiv](https://arxiv.org/abs/1908.05646)]
- On the Validity of Self-Attention as Explanation in Transformer Models [[arXiv](https://arxiv.org/abs/1908.04211)]
- Attention on Attention for Image Captioning [[arXiv](https://arxiv.org/abs/1908.06954)] [[Code](https://github.com/husthuaan/AoANet)]
- Attention is not not Explanation [[arXiv](https://arxiv.org/abs/1908.04626)] [[Code](https://github.com/sarahwie/attention)]
- One Model To Rule Them All [[arXiv](https://arxiv.org/abs/1908.03015)]
- The HSIC Bottleneck: Deep Learning without Back-Propagation [[arXiv](https://arxiv.org/abs/1908.01580)]

#### 2019-07
- ERNIE 2.0: A Continual Pre-training Framework for Language Understanding [[arXiv](https://arxiv.org/abs/1907.12412)] [[Code](https://github.com/PaddlePaddle/ERNIE)]
- RoBERTa: A Robustly Optimized BERT Pretraining Approach [[arXiv](https://arxiv.org/abs/1907.11692)] [[Code](https://github.com/pytorch/fairseq)]
- Lookahead Optimizer: k steps forward, 1 step back [[arXiv](https://arxiv.org/abs/1907.08610)]
- Automated Machine Learning in Practice: State of the Art and Recent Results [[arXiv](https://arxiv.org/abs/1907.08392)]
- OmniNet: A unified architecture for multi-modal multi-task learning [[arXiv](https://arxiv.org/abs/1907.07804)]
- Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches [[arXiv](https://arxiv.org/abs/1907.06902)] [[Code](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation)]
- A Differentiable Programming System to Bridge Machine Learning and Scientific Computing [[arXiv](https://arxiv.org/abs/1907.07587)]
- Probing Neural Network Comprehension of Natural Language Arguments [[arXiv](https://arxiv.org/abs/1907.07355)]
- Efficient Video Generation on Complex Datasets [[arXiv](https://arxiv.org/abs/1907.06571)]

#### 2019-06
- COMET: Commonsense Transformers for Automatic Knowledge Graph Construction [[arXiv](https://arxiv.org/abs/1906.05317)] [[Code](https://github.com/atcbosselut/comet-commonsense)]
- Deep Learning Recommendation Model for Personalization and Recommendation Systems [[arXiv](https://arxiv.org/abs/1906.00091)] [[Code](https://github.com/facebookresearch/dlrm)]
- Modern Deep Reinforcement Learning Algorithms [[arXiv](https://arxiv.org/abs/1906.10025)]
- Stacked Capsule Autoencoders [[arXiv](https://arxiv.org/abs/1906.06818)]
- Is Attention Interpretable? [[arXiv](https://arxiv.org/abs/1906.03731)] [[Code](https://github.com/serrano-s/attn-tests)]
- Weight Agnostic Neural Networks [[arXiv](https://arxiv.org/abs/1906.04358)]
- Generating Diverse High-Fidelity Images with VQ-VAE-2 [[arXiv](https://arxiv.org/abs/1906.00446)]
- Keeping Notes: Conditional Natural Language Generation with a Scratchpad Mechanism [[arXiv](https://arxiv.org/abs/1906.05275)]
- Visualizing and Measuring the Geometry of BERT [[arXiv](https://arxiv.org/abs/1906.02715)]
- What Does BERT Look At? An Analysis of BERT's Attention [[arXiv](https://arxiv.org/abs/1906.04341)] [[Code](https://github.com/clarkkev/attention-analysis)]
- Modern Deep Reinforcement Learning Algorithms [[arXiv](https://arxiv.org/abs/1906.10025)] [[Code](https://github.com/FortsAndMills/Learning-Reinforcement-Learning/tree/master/LRL)]
- When Does Label Smoothing Help? [[arXiv](https://arxiv.org/abs/1906.02629)]
- Large Scale Structure of Neural Network Loss Landscapes [[arXiv](https://arxiv.org/abs/1906.04724)]
- Stand-Alone Self-Attention in Vision Models [[arXiv](https://arxiv.org/abs/1906.05909)]
- Generative Adversarial Networks: A Survey and Taxonomy [[arXiv](https://arxiv.org/abs/1906.01529)]
- Weight Agnostic Neural Networks [[arXiv](https://arxiv.org/abs/1906.04358)]
- An Introduction to Variational Autoencoders [[arXiv](https://arxiv.org/abs/1906.02691)]
- XLNet: Generalized Autoregressive Pretraining for Language Understanding [[arXiv](https://arxiv.org/abs/1906.08237)] [[Code](https://github.com/zihangdai/xlnet)]
- Selfie: Self-supervised Pretraining for Image Embedding [[arXiv](https://arxiv.org/abs/1906.02940)] 

#### ACL-19
- ERNIE: Enhanced Language Representation with Informative Entities [[arXiv](https://arxiv.org/abs/1905.07129)] [[Code](https://github.com/thunlp/ERNIE)]
- Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned [[arXiv](https://arxiv.org/abs/1905.09418)] [[Code](https://github.com/lena-voita/the-story-of-heads)]
- PaperRobot: Incremental Draft Generation of Scientific Ideas [[arXiv](https://arxiv.org/abs/1905.07870)] [[Code](https://github.com/EagleW/PaperRobot)]
- BERT Rediscovers the Classical NLP Pipeline [[arXiv](https://arxiv.org/abs/1905.05950)]

#### ICML-19
- Bayesian Generative Active Deep Learning [[arXiv](https://arxiv.org/abs/1904.11643)]
- Rates of Convergence for Sparse Variational Gaussian Process Regression [[arXiv](https://arxiv.org/abs/1903.03571)]
- Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations [[arXiv](https://arxiv.org/abs/1811.12359)] [[Code](https://github.com/google-research/disentanglement_lib)]
- On Variational Bounds of Mutual Information [[arXiv](https://arxiv.org/abs/1905.06922)]
- Making Convolutional Networks Shift-Invariant Again [[arXiv](https://arxiv.org/abs/1904.11486)] [[Code](https://github.com/adobe/antialiased-cnns)]
- Similarity of Neural Network Representations Revisited [[arXiv](https://arxiv.org/abs/1905.00414)]
- MASS: Masked Sequence to Sequence Pre-training for Language Generation [[arXiv](https://arxiv.org/abs/1905.02450)] [[Code]()]

#### 2019-05
- MixMatch: A Holistic Approach to Semi-Supervised Learning [[arXiv](https://arxiv.org/abs/1905.02249)] [[Code](https://github.com/google-research/mixmatch)]
- An Explicitly Relational Neural Network Architecture [[arXiv](https://arxiv.org/abs/1905.10307)]
- A Survey on Neural Architecture Search [[arXiv](https://arxiv.org/abs/1905.01392)]
- Learning What and Where to Transfer [[arXiv](https://arxiv.org/abs/1905.05901)]
- Object Detection in 20 Years: A Survey [[arXiv](https://arxiv.org/abs/1905.05055)]
- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks [[arXiv](https://arxiv.org/abs/1905.11946)] [[Code](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)]
- Adversarial Examples Are Not Bugs, They Are Features [[arXiv](https://arxiv.org/abs/1905.02175)]

#### 2019-04
- Unsupervised Data Augmentation [[arXiv](https://arxiv.org/abs/1904.12848)] [[Code](https://github.com/google-research/uda)]
- A Closer Look at Few-shot Classification [[arXiv](https://arxiv.org/abs/1904.04232)] [[Code](https://github.com/wyharveychen/CloserLookFewShot)]
- Attention Augmented Convolutional Networks [[arXiv](https://arxiv.org/abs/1904.09925)]
- Survey on Automated Machine Learning [[arXiv](https://arxiv.org/abs/1904.12054)]
- Unifying Human and Statistical Evaluation for Natural Language Generation [[arXiv](https://arxiv.org/abs/1904.02792)] [[Code](https://github.com/hughbzhang/HUSE)]
- Generating Long Sequences with Sparse Transformers [[arXiv](https://arxiv.org/abs/1904.10509)] [[Code](https://github.com/openai/sparse_attention)]
- Neural Message Passing for Multi-Label Classification [[arXiv](https://arxiv.org/abs/1904.08049)] [[Code](https://github.com/QData/LaMP)]
- CNM: An Interpretable Complex-valued Network for Matching [[arXiv](https://arxiv.org/abs/1904.05298)] [[Code](https://github.com/wabyking/qnn)]
- What's in a Name? Reducing Bias in Bios without Access to Protected Attributes [[arXiv](https://arxiv.org/abs/1904.05233)]
- Analysing Mathematical Reasoning Abilities of Neural Models [[arXiv](https://arxiv.org/abs/1904.01557)] [[dataset](https://github.com/deepmind/mathematics_dataset)]

#### 2019-03
- To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks [[arXiv](https://arxiv.org/abs/1903.05987)]
- Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition [[arXiv](https://arxiv.org/abs/1903.10346)]
- Semantic Image Synthesis with Spatially-Adaptive Normalization [[arXiv](https://arxiv.org/abs/1903.07291)] [[Code](https://github.com/NVlabs/SPADE)]
- High-Fidelity Image Generation With Fewer Labels [[arXiv](https://arxiv.org/abs/1903.02271)] [[Code](https://github.com/google/compare_gan)]

#### NAACL-19
- CNM: An Interpretable Complex-valued Network for Matching [[arXiv](https://arxiv.org/abs/1904.05298)] [[Code](https://github.com/wabyking/qnn)]

#### 2019-02 
- A Simple Baseline for Bayesian Uncertainty in Deep Learning [[arXiv](https://arxiv.org/abs/1902.02476)] [[Code](https://github.com/wjmaddox/swa_gaussian)]
- Attention is not Explanation [[arXiv](https://arxiv.org/abs/1902.10186)] [[Code](https://github.com/successar/AttentionExplanation)]
- End-to-End Open-Domain Question Answering with BERTserini [[arXiv](https://arxiv.org/abs/1902.01718)]
- Embodied Multimodal Multitask Learning [[arXiv](https://arxiv.org/abs/1902.01385)]
- An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models [[arXiv](https://arxiv.org/abs/1902.10547)]
- Simplifying Graph Convolutional Networks [[arXiv](https://arxiv.org/abs/1902.07153)] [[Code](https://github.com/Tiiiger/SGC)]
- Superposition of many models into one [[arXiv](https://arxiv.org/abs/1902.05522)]
- Topology of Learning in Artificial Neural Networks [[arXiv](https://arxiv.org/abs/1902.08160)] [[Code](https://github.com/maximevictor/topo-learning)]
- A novel adaptive learning rate scheduler for deep neural networks [[arXiv](https://arxiv.org/abs/1902.07399)] [[Code](https://github.com/yrahul3910/adaptive-lr-dnn)]
- Bag of Freebies for Training Object Detection Neural Networks [[arXiv](https://arxiv.org/abs/1902.04103)]
- Parameter-Efficient Transfer Learning for NLP [[arXiv](https://arxiv.org/abs/1902.00751)]

#### 2019-01
- Multi-Task Deep Neural Networks for Natural Language Understanding [[arXiv](https://arxiv.org/abs/1901.11504)] [[Code](https://github.com/namisan/mt-dnn)]
- Learning from Dialogue after Deployment: Feed Yourself, Chatbot! [[arXiv](https://arxiv.org/abs/1901.05415)]
- Semi-Unsupervised Learning with Deep Generative Models: Clustering and Classifying using Ultra-Sparse Labels [[arXiv](https://arxiv.org/abs/1901.08560)] [[Code](https://github.com/SemiUnsupervisedLearning/DGMs_for_semi-unsupervised_learning)]
- Attentive Neural Processes [[arXiv](https://arxiv.org/abs/1901.05761)]
- Pay Less Attention with Lightweight and Dynamic Convolutions [[arXiv](https://arxiv.org/abs/1901.10430)]
- Cross-lingual Language Model Pretraining [[arXiv](https://arxiv.org/abs/1901.07291)] [[Code](https://github.com/facebookresearch/XLM)]
- The Evolved Transformer [[arXiv](https://arxiv.org/abs/1901.11117)]
- Glyce: Glyph-vectors for Chinese Character Representations [[arXiv](https://arxiv.org/abs/1901.10125)] 
- Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [[arXiv](https://arxiv.org/abs/1901.02860)] [[Code]()]
- Panoptic Feature Pyramid Networks [[arXiv](https://arxiv.org/abs/1901.02446)]

#### 2018-12
- A Tutorial on Deep Latent Variable Models of Natural Language [[arXiv](https://arxiv.org/abs/1812.06834)]
- On the Dimensionality of Word Embedding [[arXiv](https://arxiv.org/abs/1812.04224)] [[Code](https://github.com/ziyin-dl/word-embedding-dimensionality-selection)]
- Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling [[arXiv](https://arxiv.org/abs/1812.10860)] [[Code](https://github.com/jsalt18-sentence-repl/jiant)]
- The Design and Implementation of XiaoIce, an Empathetic Social Chatbot [[arXiv](https://arxiv.org/abs/1812.08989)]
- Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors [[arXiv](https://arxiv.org/abs/1812.08985)]
- SlowFast Networks for Video Recognition [[arXiv](https://arxiv.org/abs/1812.03982)]
- Bag of Tricks for Image Classification with Convolutional Neural Networks [[arXiv](https://arxiv.org/abs/1812.01187)] [[Code](https://github.com/dmlc/gluon-cv)]

#### ICLR-19
- RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space [[arXiv](https://arxiv.org/abs/1902.10197)] [[Code](https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding)]
- Differentiable Learning-to-Normalize via Switchable Normalization [[arXiv](https://arxiv.org/abs/1806.10779)] [[Code](https://github.com/switchablenorms/Switchable-Normalization)]
- Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset [[arXiv](https://arxiv.org/abs/1810.12247)] [[dataset](https://g.co/magenta/maestro-dataset)]
- ProMP: Proximal Meta-Policy Search [[arXiv](https://arxiv.org/abs/1810.06784)]
- Slimmable Neural Networks [[arXiv](https://arxiv.org/abs/1812.08928)] [[Code](https://github.com/JiahuiYu/slimmable_networks)]
- Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1810.09536)] [[Code](https://github.com/yikangshen/Ordered-Neurons)]
- ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA [[Code](https://github.com/xchen-tamu/alista)]
- KnockoffGAN: Generating Knockoffs for Feature Selection using Generative Adversarial Networks 
- Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations [[arXiv](https://arxiv.org/abs/1807.01697)] [[Code](https://github.com/hendrycks/robustness)]
- Rethinking the Value of Network Pruning [[arXiv](https://arxiv.org/abs/1810.05270)] [[Code](https://github.com/Eric-mingjie/rethinking-network-pruning)]
- Large Scale GAN Training for High Fidelity Natural Image Synthesis [[arXiv](https://arxiv.org/abs/1809.11096)]

#### 2018-11
- Differentiating Concepts and Instances for Knowledge Graph Embedding [[arXiv](https://arxiv.org/abs/1811.04588)]  [[Code](https://github.com/davidlvxin/TransC)]
- Gradient Harmonized Single-stage Detector [[arXiv](https://arxiv.org/abs/1811.05181)] [[Code](https://github.com/libuyu/GHM_Detection)]
- Guiding Policies with Language via Meta-Learning [[arXiv](https://arxiv.org/abs/1811.07882)]
- Dataset Distillation [[arXiv](https://arxiv.org/abs/1811.10959)]
- GAN Dissection: Visualizing and Understanding Generative Adversarial Networks [[arXiv](https://arxiv.org/abs/1811.10597)] [[Code](https://github.com/CSAILVision/gandissect)]
- Towards Explainable NLP: A Generative Explanation Framework for Text Classification [[arXiv](https://arxiv.org/abs/1811.00196)]
- Rethinking ImageNet Pre-training [[arXiv](https://arxiv.org/abs/1811.08883)]
- Sampling Can Be Faster Than Optimization [[arXiv](https://arxiv.org/abs/1811.08413)]
- Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms? [[arXiv](https://arxiv.org/abs/1811.02553)]
- Gradient Descent Finds Global Minima of Deep Neural Networks [[arXiv](https://arxiv.org/abs/1811.03804)]

#### 2018-10
- Three Mechanisms of Weight Decay Regularization [[arXiv](https://arxiv.org/abs/1810.12281)]
- Neural Nearest Neighbors Networks [[arXiv](https://arxiv.org/abs/1810.12575)] [[Code](https://github.com/visinf/n3net/)]
- Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow [[arXiv](https://arxiv.org/abs/1810.00821)] [[Code](https://github.com/akanazawa/vgan)]
- DropBlock: A regularization method for convolutional networks [[arXiv](https://arxiv.org/abs/1810.12890)]
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [[arXiv](https://arxiv.org/abs/1810.04805)][[Code](https://github.com/google-research/bert)][[Code](https://github.com/codertimo/BERT-pytorch)]
- Gradient Descent Provably Optimizes Over-parameterized Neural Networks [[arXiv](https://arxiv.org/abs/1810.02054)]

#### 2018-09
- Multi-Source Domain Adaptation with Mixture of Experts [[arXiv](https://arxiv.org/abs/1809.02256)][[Code](https://github.com/jiangfeng1124/transfer)]
- Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation [[arXiv](https://arxiv.org/abs/1809.02094)] [[Code](https://github.com/artetxem/uncovec)]
- Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding [[arXiv](https://arxiv.org/abs/1809.03702)]

#### NIPS-18
- Optimal Algorithms for Non-Smooth Distributed Optimization in Networks [[arXiv](https://arxiv.org/abs/1806.00291)]
- Non-delusional Q-learning and value-iteration
- Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes
- Neural Ordinary Differential Equations [[arXiv](https://arxiv.org/abs/1806.07366)]
- How Does Batch Normalization Help Optimization? [[arXiv](https://arxiv.org/abs/1805.11604)]
- Learning Disentangled Joint Continuous and Discrete Representations [[arXiv](https://arxiv.org/abs/1804.00104)] [[Code](https://github.com/Schlumberger/joint-vae)]
- Tree-to-tree Neural Networks for Program Translation [[arXiv](https://arxiv.org/abs/1802.03691)]
- Distilled Wasserstein Learning for Word Embedding and Topic Modeling [[arXiv](https://arxiv.org/abs/1809.04705)]
- Reversible Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1810.10999)][[Code](https://github.com/matthewjmackay/reversible-rnn)]
- Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding [[arXiv](https://arxiv.org/abs/1810.02338)]

#### 2018-08
- Understanding Back-Translation at Scale [[arXiv](https://arxiv.org/abs/1808.09381)] [[Code](https://github.com/pytorch/fairseq)]
- Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures [[arXiv](https://arxiv.org/abs/1808.08946)]
- Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction [[arXiv](https://arxiv.org/abs/1808.03867)] [[Code](https://github.com/elbayadm/attn2d)]
- Learning Neural Templates for Text Generation [[arXiv](https://arxiv.org/abs/1808.10122)] [[Code](https://github.com/harvardnlp/neural-template-gen)]
- Is Wasserstein all you need? [[arXiv](https://arxiv.org/abs/1808.09663)]
- Training Deeper Neural Machine Translation Models with Transparent Attention [[arXiv](https://arxiv.org/abs/1808.07561)]
- Contextual Parameter Generation for Universal Neural Machine Translation [[arXiv](https://arxiv.org/abs/1808.08493)][[Code](https://github.com/eaplatanios/symphony-mt)]
- CoQA: A Conversational Question Answering Challenge [[arXiv](https://arxiv.org/abs/1808.07042)] [[dataset](https://stanfordnlp.github.io/coqa/)]
- Learning deep representations by mutual information estimation and maximization [[arXiv](https://arxiv.org/abs/1808.06670)]
- Large-Scale Study of Curiosity-Driven Learning [[arXiv](https://arxiv.org/abs/1808.04355)] [[Code](https://github.com/openai/large-scale-curiosity)]
- A Study of Reinforcement Learning for Neural Machine Translation [[arXiv](https://arxiv.org/abs/1808.08866)] [[Code](https://github.com/apeterswu/RL4NMT)]

#### 2018-07
- Implementing Neural Turing Machines [[arXiv](https://arxiv.org/abs/1807.08518)] [[Code](https://github.com/MarkPKCollier/NeuralTuringMachine)]
- Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer [[arXiv](https://arxiv.org/abs/1807.07543)] [[Code](https://github.com/brain-research/acai)] [[Code](https://gist.github.com/kylemcdonald/e8ca989584b3b0e6526c0a737ed412f0)]
- Glow: Generative Flow with Invertible 1x1 Convolutions [[arXiv](https://arxiv.org/abs/1807.03039)] [[Code](https://github.com/openai/glow)]
- Representation Learning with Contrastive Predictive Coding [[arXiv](https://arxiv.org/abs/1807.03748)]
- Latent Alignment and Variational Attention [[arXiv](https://arxiv.org/abs/1807.03756)] [[Code](https://github.com/harvardnlp/var-attn/)]
- The GAN Landscape: Losses, Architectures, Regularization, and Normalization [[arXiv](https://arxiv.org/abs/1807.04720)] [[Code](https://github.com/google/compare_gan)]
- Conditional Neural Processes [[arXiv](https://arxiv.org/abs/1807.01613)]
- Neural Processes [[arXiv](https://arxiv.org/abs/1807.01622)]
- Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study [[arXiv](https://arxiv.org/abs/1807.01270)]

#### 2018-06
- Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series [[arXiv](https://arxiv.org/abs/1806.02199)] [[Code](https://github.com/ratschlab/SOM-VAE)]
- DARTS: Differentiable Architecture Search [[arXiv](https://arxiv.org/abs/1806.09055)] [[Code](https://github.com/quark0/darts)]
- Auto-Keras: Efficient Neural Architecture Search with Network Morphism [[arXiv](https://arxiv.org/abs/1806.10282)] [[Code](https://autokeras.com/)]
- Design Challenges and Misconceptions in Neural Sequence Labeling [[arXiv](https://arxiv.org/abs/1806.04470)] [[Code](https://github.com/jiesutd/NCRFpp)]
- Guided evolutionary strategies: escaping the curse of dimensionality in random search [[arXiv](https://arxiv.org/abs/1806.10230)]
- Hierarchical Graph Representation Learning with Differentiable Pooling [[arXiv](https://arxiv.org/abs/1806.08804)]
- Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering [[arXiv](https://arxiv.org/abs/1806.04330)] [[Code](https://github.com/lanwuwei/SPM_toolkit)]
- Gaussian mixture models with Wasserstein distance [[arXiv](https://arxiv.org/abs/1806.04465)]
- Relational recurrent neural networks [[arXiv](https://arxiv.org/abs/1806.01822)]
- RUDDER: Return Decomposition for Delayed Rewards [[arXiv](https://arxiv.org/abs/1806.07857)] [[Code](https://github.com/ml-jku/baselines-rudder)]
- GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations [[arXiv](https://arxiv.org/abs/1806.05662)]
- Know What You Don't Know: Unanswerable Questions for SQuAD [[arXiv](https://arxiv.org/abs/1806.03822)] [[dataset](https://rajpurkar.github.io/SQuAD-explorer/)]
- Relational inductive biases, deep learning, and graph networks [[arXiv](https://arxiv.org/abs/1806.01261)] [[Code](https://github.com/deepmind/graph_nets)]

#### ICML-18
- Fast Decoding in Sequence Models using Discrete Latent Variables [[arXiv](https://arxiv.org/abs/1803.03382)]
- Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples [[arXiv](https://arxiv.org/abs/1802.00420)] [[Code](https://github.com/anishathalye/obfuscated-gradients)]
- Delayed Impact of Fair Machine Learning [[arXiv](https://arxiv.org/abs/1803.04383)]

#### 2018-05
- Chinese NER Using Lattice LSTM [[arXiv](https://arxiv.org/abs/1805.02023)] [[Code](https://github.com/jiesutd/LatticeLSTM)]
- Transformation Networks for Target-Oriented Sentiment Classification [[arXiv](https://arxiv.org/abs/1805.01086)] [[Code](https://github.com/lixin4ever/TNet)]
- Hybrid semi-Markov CRF for Neural Sequence Labeling [[arXiv](https://arxiv.org/abs/1805.03838)] [[Code](https://github.com/ZhixiuYe/HSCRF-pytorch)]
- What you can cram into a single vector: Probing sentence embeddings for linguistic properties [[arXiv](https://arxiv.org/abs/1805.01070)] [[Code](https://github.com/facebookresearch/SentEval/tree/master/data/probing)]
- Training Classifiers with Natural Language Explanations [[arXiv](https://arxiv.org/abs/1805.03818)] [[Code](https://github.com/HazyResearch/babble)]
- Deep Reinforcement Learning For Sequence to Sequence Models [[arXiv](https://arxiv.org/abs/1805.09461)] [[Code](https://github.com/yaserkl/RLSeq2Seq)]
- Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms [[arXiv](https://arxiv.org/abs/1805.09843)] [[Code](https://github.com/dinghanshen/SWEM)]
- Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information [[arXiv](https://arxiv.org/abs/1805.04655)] [[Code](https://github.com/raosudha89/ranking_clarification_questions)]
- Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context [[arXiv](https://arxiv.org/abs/1805.04623)] [[Code](https://github.com/urvashik/lm-context-analysis)]
- AutoAugment: Learning Augmentation Policies from Data [[arXiv](https://arxiv.org/abs/1805.09501)]
- Meta-Gradient Reinforcement Learning [[arXiv](https://arxiv.org/abs/1805.09801)]
- Born Again Neural Networks [[arXiv](https://arxiv.org/abs/1805.04770)]
- Convolutional CRFs for Semantic Segmentation [[arXiv](https://arxiv.org/abs/1805.04777)]  [[Code](https://github.com/MarvinTeichmann/ConvCRF)]
- Did the Model Understand the Question? [[arXiv](https://arxiv.org/abs/1805.05492)] [[Code](https://github.com/pramodkaushik/acl18_results)]
- From Word to Sense Embeddings: A Survey on Vector Representations of Meaning [[arXiv](https://arxiv.org/abs/1805.04032)]
- Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review [[arXiv](https://arxiv.org/abs/1805.00909)]

#### 2018-04
- The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation [[arXiv](https://arxiv.org/abs/1804.09849)]
- Linguistically-Informed Self-Attention for Semantic Role Labeling [[arXiv](https://arxiv.org/abs/1804.08199)]
- Taskonomy: Disentangling Task Transfer Learning [[arXiv](https://arxiv.org/abs/1804.08328)] [[Code](https://github.com/StanfordVL/taskonomy)]
- Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation [[arXiv](https://arxiv.org/abs/1804.08069)] [[Code](https://github.com/snakeztc/NeuralDialog-LAED)]
- Learning Semantic Textual Similarity from Conversations [[arXiv](https://arxiv.org/abs/1804.07754)]
- When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation? [[arXiv](https://arxiv.org/abs/1804.06323)]
- QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension [[arXiv](https://arxiv.org/abs/1804.09541)] [[Code](https://github.com/hengruo/QANet-pytorch)]
- Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models [[arXiv](https://arxiv.org/abs/1804.09299)] [[Code](https://github.com/HendrikStrobelt/Seq2Seq-Vis)]
- Low Rank Structure of Learned Representations [[arXiv](https://arxiv.org/abs/1804.07090)]
- Decoupled Networks [[arXiv](https://arxiv.org/abs/1804.08071)]
- Learned Deformation Stability in Convolutional Neural Networks [[arXiv](https://arxiv.org/abs/1804.04438)]
- Phrase-Based & Neural Unsupervised Machine Translation [[arXiv](https://arxiv.org/abs/1804.07755)]
- Learning to Map Context-Dependent Sentences to Executable Formal Queries [[arXiv](https://arxiv.org/abs/1804.06868)] [[Code](https://github.com/clic-lab/atis)]
- Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations [[arXiv](https://arxiv.org/abs/1804.02485)]
- Differentiable plasticity: training plastic neural networks with backpropagation [[arXiv](https://arxiv.org/abs/1804.02464)]
- Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning [[arXiv](https://arxiv.org/abs/1804.00079)] [[Code](https://github.com/Maluuba/gensen)]

#### 2018-03
- An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling [[arXiv](https://arxiv.org/abs/1803.01271)] [[Code](https://github.com/locuslab/TCN)]
- Variance Networks: When Expectation Does Not Meet Your Expectations [[arXiv](https://arxiv.org/abs/1803.03764)]
- Referring Relationships [[arXiv](https://arxiv.org/abs/1803.10362)]
- Iterative Visual Reasoning Beyond Convolutions [[arXiv](https://arxiv.org/abs/1803.11189)]
- World Models [[arXiv](https://arxiv.org/abs/1803.10122)]
- Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning [[arXiv](https://arxiv.org/abs/1803.05268)]
- A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay [[arXiv](https://arxiv.org/abs/1803.09820)]
- Feudal Reinforcement Learning for Dialogue Management in Large Domains [[arXiv](https://arxiv.org/abs/1803.03232)]
- Universal Sentence Encoder [[arXiv](https://arxiv.org/abs/1803.11175)]
- Averaging Weights Leads to Wider Optima and Better Generalization [[arXiv](https://arxiv.org/abs/1803.05407)]
- On the importance of single directions for generalization [[arXiv](https://arxiv.org/abs/1803.06959)]
- Group Normalization [[arXiv](https://arxiv.org/abs/1803.08494)]
- Compositional Attention Networks for Machine Reasoning [[arXiv](https://arxiv.org/abs/1803.03067)]
- Learning Longer-term Dependencies in RNNs with Auxiliary Losses [[arXiv](https://arxiv.org/abs/1803.00144)]

#### 2018-02
- Regularized Evolution for Image Classifier Architecture Search [[arXiv](https://arxiv.org/abs/1802.01548)]
- Visual Interpretability for Deep Learning: a Survey [[arXiv](https://arxiv.org/abs/1802.00614)]
- Deep contextualized word representation [[arXiv](https://arxiv.org/abs/1802.05365)]
- Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs [[arXiv](https://arxiv.org/abs/1802.10026)] [[Code](https://github.com/timgaripov/dnn-mode-connectivity)]
- Machine Theory of Mind [[arXiv](https://arxiv.org/abs/1802.07740)] 
- Efficient Neural Architecture Search via Parameter Sharing [[arXiv](https://arxiv.org/abs/1802.03268)]
- Interpreting CNNs via Decision Trees [[arXiv](https://arxiv.org/abs/1802.00121)]
- DensePose: Dense Human Pose Estimation In The Wild [[arXiv](https://arxiv.org/abs/1802.00434)] [[dataset](http://densepose.org/)]
- DeepType: Multilingual Entity Linking by Neural Type System Evolution [[arXiv](https://arxiv.org/abs/1802.01021)] [[Code](https://github.com/openai/deeptype)]
- Recent Advances in Neural Program Synthesis [[arXiv](https://arxiv.org/abs/1802.02353)]
- IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [[arXiv](https://arxiv.org/abs/1802.01561)] [[Code](https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/dmlab30)]
- Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples [[arXiv](https://arxiv.org/abs/1802.00420)] [[Code](https://github.com/anishathalye/obfuscated-gradients)]

#### 2018-01
- Universal Language Model Fine-tuning for Text Classification [[arXiv](https://arxiv.org/abs/1801.06146)]
- Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling [[arXiv](https://arxiv.org/abs/1801.10296)]
- Quantum Computing in the NISQ era and beyond [[arXiv](https://arxiv.org/abs/1801.00862)]
- Active Neural Localization [[arXiv](https://arxiv.org/abs/1801.08214)]
- Global overview of Imitation Learning [[arXiv](https://arxiv.org/abs/1801.06503)]
- MaskGAN: Better Text Generation via Filling in the ______ [[arXiv](https://arxiv.org/abs/1801.07736)] [[Code](https://github.com/tensorflow/models/tree/master/research/maskgan)]
- Deep Learning for Sentiment Analysis : A Survey [[arXiv](https://arxiv.org/abs/1801.07883)]
- Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift [[arXiv](https://arxiv.org/abs/1801.05134)]
- DENSER: Deep Evolutionary Network Structured Representation [[arXiv](https://arxiv.org/abs/1801.01563)]
- Building a Conversational Agent Overnight with Dialogue Self-Play [[arXiv](https://arxiv.org/abs/1801.04871)]
- Unsupervised Real-to-Virtual Domain Unification for End-to-End Highway Driving [[arXiv](https://arxiv.org/abs/1801.03458)]
- SBNet: Sparse Blocks Network for Fast Inference [[arXiv](https://arxiv.org/abs/1801.02108)]
- Deep Stereo Matching with Explicit Cost Aggregation Sub-Architecture [[arXiv](https://arxiv.org/abs/1801.04065)]
- Adversarial Spheres [[arXiv](https://arxiv.org/abs/1801.02774)]
- Neural Program Synthesis with Priority Queue Training [[arXiv](https://arxiv.org/abs/1801.03526)] [[Code](https://github.com/tensorflow/models/tree/master/research/brain_coder)]
- Adversarial Generative Nets: Neural Network Attacks on State-of-the-Art Face Recognition [[arXiv](https://arxiv.org/abs/1801.00349)]
- DeepMind Control Suite [[arXiv](https://arxiv.org/abs/1801.00690)] [[dataset](https://github.com/deepmind/dm_control)]

#### 2017-12
- Noisy Natural Gradient as Variational Inference [[arXiv](https://arxiv.org/abs/1712.02390)] [[Code](https://github.com/wlwkgus/NoisyNaturalGradient)]
- Non-convex Optimization for Machine Learning [[arXiv](https://arxiv.org/abs/1712.07897)]
- Improving Generalization Performance by Switching from Adam to SGD [[arXiv](https://arxiv.org/abs/1712.07628)]
- Learning by Asking Questions [[arXiv](https://arxiv.org/abs/1712.01238)]
- A Flexible Approach to Automated RNN Architecture Generation [[arXiv](https://arxiv.org/abs/1712.07316)]
- Lectures on Randomized Numerical Linear Algebra [[arXiv](https://arxiv.org/abs/1712.08880)]
- Data Distillation: Towards Omni-Supervised Learning [[arXiv](https://arxiv.org/abs/1712.04440)]
- Deep Learning Scaling is Predictable, Empirically [[arXiv](https://arxiv.org/abs/1712.00409)]
- SGAN: An Alternative Training of Generative Adversarial Networks [[arXiv](https://arxiv.org/abs/1712.02330)]
- The NarrativeQA Reading Comprehension Challenge [[arXiv](https://arxiv.org/abs/1712.07040)] [[dataset](https://github.com/deepmind/narrativeqa)]
- Regularization and Optimization strategies in Deep Convolutional Neural Network [[arXiv](https://arxiv.org/abs/1712.04711)]
- Mathematics of Deep Learning [[arXiv](https://arxiv.org/abs/1712.04741)]
- Text Generation Based on Generative Adversarial Nets with Latent Variable [[arXiv](https://arxiv.org/abs/1712.00170)]
- Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
- Show-and-Fool: Crafting Adversarial Examples for Neural Image Captioning [[arXiv](https://arxiv.org/abs/1712.02051)] [[Code](https://github.com/huanzhang12/ImageCaptioningAttack)]

#### ICLR-18
- Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling
- i-RevNet: Deep Invertible Networks [[arXiv](https://arxiv.org/abs/1802.07088)] [[Code](https://github.com/jhjacobsen/pytorch-i-revnet)]
- Hierarchical Representations for Efficient Architecture Search [[arXiv](https://arxiv.org/abs/1711.00436)]
- Backpropagation through the Void: Optimizing control variates for black-box gradient estimation [[arXiv](https://arxiv.org/abs/1711.00123)] [[Code](https://github.com/duvenaud/relax)]
- Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection
- Wasserstein Auto-Encoders [[arXiv](https://arxiv.org/abs/1711.01558)]
- Neural Speed Reading via Skim-RNN [[arXiv](https://arxiv.org/abs/1711.02085)]
- Breaking the Softmax Bottleneck: A High-Rank RNN Language Model [[arXiv](https://arxiv.org/abs/1711.03953)] [[Code](https://github.com/zihangdai/mos)]
- Non-Autoregressive Neural Machine Translation [[arXiv](https://arxiv.org/abs/1711.02281)]
- A DIRT-T Approach to Unsupervised Domain Adaptation
- On the Information Bottleneck Theory of Deep Learning
- mixup: Beyond Empirical Risk Minimization
- Unsupervised Machine Translation Using Monolingual Corpora Only [[arXiv](https://arxiv.org/abs/1711.00043)]
- Matrix capsules with EM routing

#### 2017-11
- Hierarchical Representations for Efficient Architecture Search [[arXiv](https://arxiv.org/abs/1711.00436)]
- Embedding Words as Distributions with a Bayesian Skip-gram Model [[arXiv](https://arxiv.org/abs/1711.11027)]
- Deep Image Prior [[arXiv](https://arxiv.org/abs/1711.10925)] [[Code](https://github.com/DmitryUlyanov/deep-image-prior)]
- MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1711.06788)]
- Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning [[arXiv](https://arxiv.org/abs/1711.07613)]
- Neural Text Generation: A Practical Guide [[arXiv](https://arxiv.org/abs/1711.09534)]
- Memory Aware Synapses: Learning what (not) to forget [[arXiv](https://arxiv.org/abs/1711.09601)]
- Are GANs Created Equal? A Large-Scale Study [[arXiv](https://arxiv.org/abs/1711.10337)]
- Distilling a Neural Network Into a Soft Decision Tree [[arXiv](https://arxiv.org/abs/1711.09784)]
- SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability [[arXiv](https://arxiv.org/abs/1706.05806)] [[Code](https://github.com/google/svcca)]
- Non-local Neural Networks [[arXiv](https://arxiv.org/abs/1711.07971)]
- Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations [[arXiv](https://arxiv.org/abs/1711.05732)] [[dataset](https://github.com/jwieting)] [[Code](https://github.com/jwieting)]
- Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks [[arXiv](https://arxiv.org/abs/1711.00350)] [[dataset](https://github.com/brendenlake/SCAN)]
- Neural Discrete Representation Learning [[arXiv](https://arxiv.org/abs/1711.00937)]
- Weighted Transformer Network for Machine Translation [[arXiv](https://arxiv.org/abs/1711.02132)]

#### 2017-10
- Dynamic Routing Between Capsules [[arXiv](https://arxiv.org/abs/1710.09829)]
- Unsupervised Neural Machine Translation [[arXiv](https://arxiv.org/abs/1710.11041)]

#### 2017-09
- Empower Sequence Labeling with Task-Aware Neural Language Model [[arXiv](https://arxiv.org/abs/1709.04109)] [[Code](https://github.com/LiyuanLucasLiu/LM-LSTM-CRF)]
- Dynamic Evaluation of Neural Sequence Models [[arXiv](https://arxiv.org/abs/1709.07432)] [[Code](https://github.com/benkrause/dynamic-evaluation)]

#### NIPS-17
- Improved Training of Wasserstein GANs [[arXiv](https://arxiv.org/abs/1704.00028)] [[Code](https://github.com/igul222/improved_wgan_training)] [[Code](https://github.com/caogang/wgan-gp)]
- The Reversible Residual Network: Backpropagation Without Storing Activations [[arXiv](https://arxiv.org/abs/1707.04585)] [[Code](https://github.com/renmengye/revnet-public)] [[Code](https://github.com/tbung/pytorch-revnet)]
- Plan, Attend, Generate: Planning for Sequence-to-Sequence Models [[arXiv](https://arxiv.org/abs/1711.10462)]
- REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models [[arXiv](https://arxiv.org/abs/1703.07370)] [[Code](https://github.com/tensorflow/models/tree/master/research/rebar)]
- Poincaré Embeddings for Learning Hierarchical Representations [[arXiv](https://arxiv.org/abs/1705.08039)]
- Character-Level Language Modeling with Recurrent Highway Hypernetworks [[Code](https://github.com/jsuarez5341/Recurrent-Highway-Hypernetworks-NIPS)] 
- Dilated Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1710.02224)] [[Code](https://github.com/code-terminator/DilatedRNN)]
- Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning [[arXiv](https://arxiv.org/abs/1711.01577)]
- Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations [[arXiv](https://arxiv.org/abs/1704.00648)]
- Learned in Translation: Contextualized Word Vectors [[arXiv](https://arxiv.org/abs/1708.00107)] [[Code](https://github.com/salesforce/cove)]

#### 2017-08
- Focal Loss for Dense Object Detection [[arXiv](https://arxiv.org/abs/1708.02002)]
- Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge [[arXiv](https://arxiv.org/abs/1708.02711)] [[Code](https://github.com/hengyuan-hu/bottom-up-attention-vqa)]

#### 2017-07
- Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering [[arXiv](https://arxiv.org/abs/1707.07998)] [[Code](https://github.com/hengyuan-hu/bottom-up-attention-vqa)]

#### 2017-06
- Self-Normalizing Neural Networks [[arXiv](https://arxiv.org/abs/1706.02515)]
- One Model To Learn Them All [[arXiv](https://arxiv.org/abs/1706.05137)] [[Code](https://github.com/tensorflow/tensor2tensor)]
- Attention Is All You Need [[arXiv](https://arxiv.org/abs/1706.03762)] [[Code](https://github.com/tensorflow/tensor2tensor)]
- A simple neural network module for relational reasoning [[arXiv](https://arxiv.org/abs/1706.01427)]

#### ICML-17
- Large-Scale Evolution of Image Classifiers [[arXiv](https://arxiv.org/abs/1703.01041)]
- Improved Variational Autoencoders for Text Modeling using Dilated Convolutions [[arXiv](https://arxiv.org/abs/1702.08139)]
- Conditional Image Synthesis With Auxiliary Classifier GANs [[arXiv](https://arxiv.org/abs/1610.09585)]
- Toward Controlled Generation of Text [[arXiv](https://arxiv.org/abs/1703.00955)]

#### 2017-05
- Ask the Right Questions: Active Question Reformulation with Reinforcement Learning [[arXiv](https://arxiv.org/abs/1705.07830)]
- Supervised Learning of Universal Sentence Representations from Natural Language Inference Data [[arXiv](https://arxiv.org/abs/1705.02364)] [[Code](https://github.com/facebookresearch/InferSent)] [[Code](https://github.com/facebookresearch/SentEval)]
- Learning to Ask: Neural Question Generation for Reading Comprehension [[arXiv](https://arxiv.org/abs/1705.00106)] [[Code](https://github.com/xinyadu/nqg)]
- Adversarial Ranking for Language Generation [[arXiv](https://arxiv.org/abs/1705.11001)]
- Learning Structured Text Representations [[arXiv](https://arxiv.org/abs/1705.09207)] [[Code](https://github.com/nlpyang/structured)]
- TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension [[arXiv](https://arxiv.org/abs/1705.03551)] [[dataset](http://nlp.cs.washington.edu/triviaqa/)] [[Code](https://github.com/mandarjoshi90/triviaqa)]
- ParlAI: A Dialog Research Software Platform [[arXiv](https://arxiv.org/abs/1705.06476)] [[Code](https://github.com/facebookresearch/ParlAI)]

#### 2017-04
- Stochastic Gradient Descent as Approximate Bayesian Inference [[arXiv](https://arxiv.org/abs/1704.04289)]
- Snapshot Ensembles: Train 1, get M for free [[arXiv](https://arxiv.org/abs/1704.00109)] [[Code](https://github.com/gaohuang/SnapshotEnsemble)] [[Code](https://github.com/titu1994/Snapshot-Ensembles)]
- From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood [[arXiv](https://arxiv.org/abs/1704.07926)] [[Code](https://github.com/kelvinguu/lang2program)]
- Reading Wikipedia to Answer Open-Domain Questions [[arXiv](https://arxiv.org/abs/1704.00051)] [[Code](https://github.com/facebookresearch/DrQA)]
- Learning to Skim Text [[arXiv](https://arxiv.org/abs/1704.06877)]

#### 2017-03
- Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [[arXiv](https://arxiv.org/abs/1703.10593)] [[Code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)]
- Evolution Strategies as a Scalable Alternative to Reinforcement Learning [[arXiv](https://arxiv.org/abs/1703.03864)] [[Code](https://github.com/openai/evolution-strategies-starter)] [[Code](https://github.com/atgambardella/pytorch-es)]

#### 2017-02
- Exploring loss function topology with cyclical learning rates [[arXiv](https://arxiv.org/abs/1702.04283)] [[Code](https://github.com/lnsmith54/exploring-loss)]
- A Hybrid Convolutional Variational Autoencoder for Text Generation [[arXiv](https://arxiv.org/abs/1702.02390)] [[Code](https://github.com/stas-semeniuta/textvae)]

#### 2017-01
- Wasserstein GAN [[arXiv](https://arxiv.org/abs/1701.07875)] [[Code](https://github.com/martinarjovsky/WassersteinGAN)]
- Adversarial Learning for Neural Dialogue Generation [[arXiv](https://arxiv.org/abs/1701.06547)]
- Deep Reinforcement Learning: An Overview [[arXiv](https://arxiv.org/abs/1701.07274)]
- OpenNMT: Open-Source Toolkit for Neural Machine Translation [[arXiv](https://arxiv.org/abs/1701.02810)] [[Code](https://github.com/OpenNMT/OpenNMT)] [[Code](https://github.com/OpenNMT/OpenNMT-py)]

#### ICLR-17
- Adversarial Training Methods for Semi-Supervised Text Classification [[arXiv](https://arxiv.org/abs/1605.07725)] [[Code](https://github.com/tensorflow/models/tree/master/research/adversarial_text)]
- Structured Attention Networks [[arXiv](https://arxiv.org/abs/1702.00887)] [[Code](https://github.com/harvardnlp/struct-attn)]
- Learning End-to-End Goal-Oriented Dialog [[arXiv](https://arxiv.org/abs/1605.07683)] [[dataset](http://fb.ai/babi)]
- Understanding deep learning requires rethinking generalization [[arXiv](https://arxiv.org/abs/1611.03530)]
- An Actor-Critic Algorithm for Sequence Prediction [[arXiv](https://arxiv.org/abs/1607.07086)]
- Learning to Remember Rare Events [[arXiv](https://arxiv.org/abs/1703.03129)]
- Introspection: Accelerating Neural Network Training By Learning Weight Evolution [[arXiv](https://arxiv.org/abs/1704.04959)]

#### 2016-11
- Categorical Reparameterization with Gumbel-Softmax [[arXiv](https://arxiv.org/abs/1611.01144)]
- A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks [[arXiv](https://arxiv.org/abs/1611.01587)]
- Image-to-Image Translation with Conditional Adversarial Networks [[arXiv](https://arxiv.org/abs/1611.07004)] [[Code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)]

#### 2016-10
- Using Fast Weights to Attend to the Recent Past [[arXiv](https://arxiv.org/abs/1610.06258)]

#### 2016-09
- HyperNetworks [[arXiv](https://arxiv.org/abs/1609.09106)]
- Language as a Latent Variable: Discrete Generative Models for Sentence Compression [[arXiv](https://arxiv.org/abs/1609.07317)]
- SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient [[arXiv](https://arxiv.org/abs/1609.05473)] [[Code](https://github.com/LantaoYu/SeqGAN)]

#### 2016-08
- Densely Connected Convolutional Networks [[arXiv](https://arxiv.org/abs/1608.06993)] [[Code](https://github.com/liuzhuang13/DenseNet)]
- SGDR: Stochastic Gradient Descent with Warm Restarts [[arXiv](https://arxiv.org/abs/1608.03983)] [[Code](https://github.com/loshchil/SGDR)]

#### 2016-07
- Enriching Word Vectors with Subword Information [[arXiv](https://arxiv.org/abs/1607.04606)] [[Code](https://github.com/facebookresearch/fastText)]
- Bag of Tricks for Efficient Text Classification [[arXiv](https://arxiv.org/abs/1607.01759)] [[Code](https://github.com/facebookresearch/fastText)]

#### 2016-06
- A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task [[arXiv](https://arxiv.org/abs/1606.02858)]
- Convolution by Evolution: Differentiable Pattern Producing Networks [[arXiv](https://arxiv.org/abs/1606.02580)]
- Conditional Image Generation with PixelCNN Decoders [[arXiv](https://arxiv.org/abs/1606.05328)]
- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets [[arXiv](https://arxiv.org/abs/1606.03657)]

#### 2016-04
- Benchmarking Deep Reinforcement Learning for Continuous Control [[arXiv](https://arxiv.org/abs/1604.06778)] [[Code](https://github.com/rll/rllab)]

#### 2016-03
- Text Understanding with the Attention Sum Reader Network [[arXiv](https://arxiv.org/abs/1603.01547)]

#### 2016-02
- Associative Long Short-Term Memory [[arXiv](https://arxiv.org/abs/1602.03032)]

#### 2015-11
- Neural Variational Inference for Text Processing [[arXiv](https://arxiv.org/abs/1511.06038)]
- Generating Sentences from a Continuous Space [[arXiv](https://arxiv.org/abs/1511.06349)]

#### 2015-06
- Cyclical Learning Rates for Training Neural Networks [[arXiv](https://arxiv.org/abs/1506.01186)] [[Code](https://github.com/bckenstler/CLR)]
- Skip-Thought Vectors [[arXiv](https://arxiv.org/abs/1506.06726)]

#### 2015-05
- U-Net: Convolutional Networks for Biomedical Image Segmentation [[arXiv](https://arxiv.org/abs/1505.04597)]

#### 2015-02
- Trust Region Policy Optimization [[arXiv](https://arxiv.org/abs/1502.05477)]

#### 2014-12
- Adam: A Method for Stochastic Optimization [[arXiv](https://arxiv.org/abs/1412.6980)]

#### 2014-11
- Conditional Generative Adversarial Nets [[arXiv](https://arxiv.org/abs/1411.1784)]

#### 2014-08
- Convolutional Neural Networks for Sentence Classification [[arXiv](https://arxiv.org/abs/1408.5882)]

#### 2014-06
- Generative Adversarial Networks [[arXiv](https://arxiv.org/abs/1406.2661)]
- Semi-Supervised Learning with Deep Generative Models [[arXiv](https://arxiv.org/abs/1406.5298)] [[Code](https://github.com/dpkingma/nips14-ssl)]

#### 2013-12
- Auto-Encoding Variational Bayes [[arXiv](https://arxiv.org/abs/1312.6114)]

#### 2012-06
- Variational Bayesian Inference with Stochastic Search [[arXiv](https://arxiv.org/abs/1206.6430)]
