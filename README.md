# papernotes
#### 2019-02 
- Bag of Freebies for Training Object Detection Neural Networks [[arXiv](https://arxiv.org/abs/1902.04103)]
- Parameter-Efficient Transfer Learning for NLP [[arXiv](https://arxiv.org/abs/1902.00751v1)]

#### 2019-01
- Multi-Task Deep Neural Networks for Natural Language Understanding [[arXiv](https://arxiv.org/abs/1901.11504)]
- Learning from Dialogue after Deployment: Feed Yourself, Chatbot! [[arXiv](https://arxiv.org/abs/1901.05415v2)]
- Semi-Unsupervised Learning with Deep Generative Models: Clustering and Classifying using Ultra-Sparse Labels [[arXiv](https://arxiv.org/abs/1901.08560v1)]
- Learning and Evaluating General Linguistic Intelligence [[arXiv](https://arxiv.org/abs/1901.11373v1)]
- Identifying and Correcting Label Bias in Machine Learning [[arXiv](https://arxiv.org/abs/1901.04966v1)]
- Attentive Neural Processes [[arXiv](https://arxiv.org/abs/1901.05761v1)]
- Pay Less Attention with Lightweight and Dynamic Convolutions [[arXiv](https://arxiv.org/abs/1901.10430v1)]
- Elimination of All Bad Local Minima in Deep Learning [[arXiv](https://arxiv.org/abs/1901.00279)]
- Cross-lingual Language Model Pretraining [[arXiv](https://arxiv.org/abs/1901.07291)] [[Code](https://github.com/facebookresearch/XLM)]
- The Evolved Transformer [[arXiv](https://arxiv.org/abs/1901.11117)]
- Glyce: Glyph-vectors for Chinese Character Representations [[arXiv](https://arxiv.org/abs/1901.10125)] 
- Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [[arXiv](https://arxiv.org/abs/1901.02860v1)] [[Code]()]
- Panoptic Feature Pyramid Networks [[arXiv](https://arxiv.org/abs/1901.02446v1)]

#### 2018-12
- Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond [[arXiv](https://arxiv.org/abs/1812.10464v1)]
- Learning Not to Learn: Training Deep Neural Networks with Biased Data [[arXiv](https://arxiv.org/abs/1812.10352v1)]
- An introduction to domain adaptation and transfer learning [[arXiv](https://arxiv.org/abs/1812.11806v2)]
- A Tutorial on Deep Latent Variable Models of Natural Language [[arXiv](https://arxiv.org/abs/1812.06834v2)]
- On the Dimensionality of Word Embedding [[arXiv](https://arxiv.org/abs/1812.04224)]
- Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling [[arXiv](https://arxiv.org/abs/1812.10860)]
- The Design and Implementation of XiaoIce, an Empathetic Social Chatbot [[arXiv](https://arxiv.org/abs/1812.08989)]
- Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors [[arXiv](https://arxiv.org/abs/1812.08985)]
- SlowFast Networks for Video Recognition [[arXiv](https://arxiv.org/abs/1812.03982)]
- Bag of Tricks for Image Classification with Convolutional Neural Networks [[arXiv](https://arxiv.org/abs/1812.01187v2)]

#### ICLR-19
- Differentiable Learning-to-Normalize via Switchable Normalization [[arXiv](https://arxiv.org/abs/1806.10779)]
- Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset [[arXiv](https://arxiv.org/abs/1810.12247)]
- ProMP: Proximal Meta-Policy Search [[arXiv](https://arxiv.org/abs/1810.06784)]
- Slimmable Neural Networks
- Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1810.09536)]
- ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA 
- KnockoffGAN: Generating Knockoffs for Feature Selection using Generative Adversarial Networks 
- Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations [[arXiv](https://arxiv.org/abs/1807.01697)]
- Rethinking the Value of Network Pruning [[arXiv](https://arxiv.org/abs/1810.05270)]
- Shallow Learning For Deep Networks 
- Large Scale GAN Training for High Fidelity Natural Image Synthesis [[arXiv](https://arxiv.org/abs/1809.11096)]

#### 2018-11
- Gradient Harmonized Single-stage Detector [[arXiv](https://arxiv.org/abs/1811.05181)] [[Code](https://github.com/libuyu/GHM_Detection)]
- Few-Shot Generalization Across Dialogue Tasks [[arXiv](https://arxiv.org/abs/1811.11707v1)]
- Unsupervised Transfer Learning for Spoken Language Understanding in Intelligent Agents [[arXiv](https://arxiv.org/abs/1811.05370v1)]
- From Recognition to Cognition: Visual Commonsense Reasoning [[arXiv](https://arxiv.org/abs/1811.10830v1)]
- Guiding Policies with Language via Meta-Learning [[arXiv](https://arxiv.org/abs/1811.07882v1)]
- Dataset Distillation [[arXiv](https://arxiv.org/abs/1811.10959v1)]
- Deformable ConvNets v2: More Deformable, Better Results [[arXiv](https://arxiv.org/abs/1811.11168v2)]
- GAN Dissection: Visualizing and Understanding Generative Adversarial Networks [[arXiv](https://arxiv.org/abs/1811.10597v1)]
- Towards Explainable NLP: A Generative Explanation Framework for Text Classification [[arXiv](https://arxiv.org/abs/1811.00196v1)]
- Automatic Paper Summary Generation from Visual and Textual Information [[arXiv](https://arxiv.org/abs/1811.06943v1)]
- Modular Architecture for StarCraft II with Deep Reinforcement Learning [[arXiv](https://arxiv.org/abs/1811.03555)]
- Rethinking ImageNet Pre-training [[arXiv](https://arxiv.org/abs/1811.08883)]
- Sampling Can Be Faster Than Optimization [[arXiv](https://arxiv.org/abs/1811.08413)]
- Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms? [[arXiv](https://arxiv.org/abs/1811.02553)]
- Gradient Descent Finds Global Minima of Deep Neural Networks [[arXiv](https://arxiv.org/abs/1811.03804)]

#### 2018-10
- Three Mechanisms of Weight Decay Regularization [[arXiv](https://arxiv.org/abs/1810.12281v1)]
- Neural Nearest Neighbors Networks [[arXiv](https://arxiv.org/abs/1810.12575v1)] [[Code](https://github.com/visinf/n3net/)]
- Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow [[arXiv](https://arxiv.org/abs/1810.00821)] [[Code](https://github.com/akanazawa/vgan)]
- DropBlock: A regularization method for convolutional networks [[arXiv](https://arxiv.org/abs/1810.12890)]
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [[arXiv](https://arxiv.org/abs/1810.04805)][[Code](https://github.com/google-research/bert)][[Code](https://github.com/codertimo/BERT-pytorch)]
- Gradient Descent Provably Optimizes Over-parameterized Neural Networks [[arXiv](https://arxiv.org/abs/1810.02054)]

#### 2018-09
- Multi-Source Domain Adaptation with Mixture of Experts [[arXiv](https://arxiv.org/abs/1809.02256)][[Code](https://github.com/jiangfeng1124/transfer)]
- Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation [[arXiv](https://arxiv.org/abs/1809.02094)] [[Code](https://github.com/artetxem/uncovec)]
- Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding [[arXiv](https://arxiv.org/abs/1809.03702)]

#### NIPS-18
- Optimal Algorithms for Non-Smooth Distributed Optimization in Networks [[arXiv](https://arxiv.org/abs/1806.00291)]
- Non-delusional Q-learning and value-iteration
- Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes
- Neural Ordinary Differential Equations [[arXiv](https://arxiv.org/abs/1806.07366)]
- How Does Batch Normalization Help Optimization? [[arXiv](https://arxiv.org/abs/1805.11604)]
- Learning Disentangled Joint Continuous and Discrete Representations [[arXiv](https://arxiv.org/abs/1804.00104)] [[Code](https://github.com/Schlumberger/joint-vae)]
- Tree-to-tree Neural Networks for Program Translation [[arXiv](https://arxiv.org/abs/1802.03691)]
- Distilled Wasserstein Learning for Word Embedding and Topic Modeling [[arXiv](https://arxiv.org/abs/1809.04705)]
- Reversible Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1810.10999v1)][[Code](https://github.com/matthewjmackay/reversible-rnn)]
- Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding [[arXiv](https://arxiv.org/abs/1810.02338)]

#### 2018-08
- Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures [[arXiv](https://arxiv.org/abs/1808.08946v3)]
- Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction [[arXiv](https://arxiv.org/abs/1808.03867v3)] [[Code](https://github.com/elbayadm/attn2d)]
- Learning Neural Templates for Text Generation [[arXiv](https://arxiv.org/abs/1808.10122)] [[Code](https://github.com/harvardnlp/neural-template-gen)]
- Is Wasserstein all you need? [[arXiv](https://arxiv.org/abs/1808.09663)]
- Training Deeper Neural Machine Translation Models with Transparent Attention [[arXiv](https://arxiv.org/abs/1808.07561)]
- Contextual Parameter Generation for Universal Neural Machine Translation [[arXiv](https://arxiv.org/abs/1808.08493)][[Code](https://github.com/eaplatanios/symphony-mt)]
- CoQA: A Conversational Question Answering Challenge [[arXiv](https://arxiv.org/abs/1808.07042)] [[dataset](https://stanfordnlp.github.io/coqa/)]
- Learning deep representations by mutual information estimation and maximization [[arXiv](https://arxiv.org/abs/1808.06670)]
- Large-Scale Study of Curiosity-Driven Learning [[arXiv](https://arxiv.org/abs/1808.04355)] [[Code](https://github.com/openai/large-scale-curiosity)]
- A Study of Reinforcement Learning for Neural Machine Translation [[arXiv](https://arxiv.org/abs/1808.08866)] [[Code](https://github.com/apeterswu/RL4NMT)]

#### 2018-07
- Implementing Neural Turing Machines [[arXiv](https://arxiv.org/abs/1807.08518)] [[Code](https://github.com/MarkPKCollier/NeuralTuringMachine)]
- Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer [[arXiv](https://arxiv.org/abs/1807.07543)] [[Code](https://github.com/brain-research/acai)] [[Code](https://gist.github.com/kylemcdonald/e8ca989584b3b0e6526c0a737ed412f0)]
- Glow: Generative Flow with Invertible 1x1 Convolutions [[arXiv](https://arxiv.org/abs/1807.03039)] [[Code](https://github.com/openai/glow)]
- Representation Learning with Contrastive Predictive Coding [[arXiv](https://arxiv.org/abs/1807.03748)]
- Latent Alignment and Variational Attention [[arXiv](https://arxiv.org/abs/1807.03756)] [[Code](https://github.com/harvardnlp/var-attn/)]
- The GAN Landscape: Losses, Architectures, Regularization, and Normalization [[arXiv](https://arxiv.org/abs/1807.04720)] [[Code](https://github.com/google/compare_gan)]
- Conditional Neural Processes [[arXiv](https://arxiv.org/abs/1807.01613)]
- Neural Processes [[arXiv](https://arxiv.org/abs/1807.01622)]
- Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study [[arXiv](https://arxiv.org/abs/1807.01270)]

#### 2018-06
- Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series [[arXiv](https://arxiv.org/abs/1806.02199)] [[Code](https://github.com/ratschlab/SOM-VAE)]
- DARTS: Differentiable Architecture Search [[arXiv](https://arxiv.org/abs/1806.09055v1)] [[Code](https://github.com/quark0/darts)]
- Auto-Keras: Efficient Neural Architecture Search with Network Morphism [[arXiv](https://arxiv.org/abs/1806.10282)] [[Code](https://autokeras.com/)]
- Design Challenges and Misconceptions in Neural Sequence Labeling [[arXiv](https://arxiv.org/abs/1806.04470)] [[Code](https://github.com/jiesutd/NCRFpp)]
- Guided evolutionary strategies: escaping the curse of dimensionality in random search [[arXiv](https://arxiv.org/abs/1806.10230)]
- Hierarchical Graph Representation Learning with Differentiable Pooling [[arXiv](https://arxiv.org/abs/1806.08804)]
- Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering [[arXiv](https://arxiv.org/abs/1806.04330)] [[Code](https://github.com/lanwuwei/SPM_toolkit)]
- Gaussian mixture models with Wasserstein distance [[arXiv](https://arxiv.org/abs/1806.04465)]
- Relational recurrent neural networks [[arXiv](https://arxiv.org/abs/1806.01822)]
- RUDDER: Return Decomposition for Delayed Rewards [[arXiv](https://arxiv.org/abs/1806.07857)] [[Code](https://github.com/ml-jku/baselines-rudder)]
- GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations [[arXiv](https://arxiv.org/abs/1806.05662)]
- Know What You Don't Know: Unanswerable Questions for SQuAD [[arXiv](https://arxiv.org/abs/1806.03822)] [[dataset](https://rajpurkar.github.io/SQuAD-explorer/)]
- Relational inductive biases, deep learning, and graph networks [[arXiv](https://arxiv.org/abs/1806.01261)] [[Code](https://github.com/deepmind/graph_nets)]

#### ICML-18
- Fast Decoding in Sequence Models using Discrete Latent Variables [[arXiv](https://arxiv.org/abs/1803.03382)]
- Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples [[arXiv](https://arxiv.org/abs/1802.00420)] [[Code](https://github.com/anishathalye/obfuscated-gradients)]
- Delayed Impact of Fair Machine Learning [[arXiv](https://arxiv.org/abs/1803.04383)]

#### 2018-05
- Chinese NER Using Lattice LSTM [[arXiv](https://arxiv.org/abs/1805.02023)] [[Code](https://github.com/jiesutd/LatticeLSTM)]
- Transformation Networks for Target-Oriented Sentiment Classification [[arXiv](https://arxiv.org/abs/1805.01086)] [[Code](https://github.com/lixin4ever/TNet)]
- Hybrid semi-Markov CRF for Neural Sequence Labeling [[arXiv](https://arxiv.org/abs/1805.03838)] [[Code](https://github.com/ZhixiuYe/HSCRF-pytorch)]
- What you can cram into a single vector: Probing sentence embeddings for linguistic properties [[arXiv](https://arxiv.org/abs/1805.01070)] [[Code](https://github.com/facebookresearch/SentEval/tree/master/data/probing)]
- Training Classifiers with Natural Language Explanations [[arXiv](https://arxiv.org/abs/1805.03818)] [[Code](https://github.com/HazyResearch/babble)]
- Deep Reinforcement Learning For Sequence to Sequence Models [[arXiv](https://arxiv.org/abs/1805.09461)] [[Code](https://github.com/yaserkl/RLSeq2Seq)]
- Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms [[arXiv](https://arxiv.org/abs/1805.09843)] [[Code](https://github.com/dinghanshen/SWEM)]
- Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information [[arXiv](https://arxiv.org/abs/1805.04655)] [[Code](https://github.com/raosudha89/ranking_clarification_questions)]
- Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context [[arXiv](https://arxiv.org/abs/1805.04623)] [[Code](https://github.com/urvashik/lm-context-analysis)]
- AutoAugment: Learning Augmentation Policies from Data [[arXiv](https://arxiv.org/abs/1805.09501)]
- Meta-Gradient Reinforcement Learning [[arXiv](https://arxiv.org/abs/1805.09801)]
- Born Again Neural Networks [[arXiv](https://arxiv.org/abs/1805.04770)]
- Convolutional CRFs for Semantic Segmentation [[arXiv](https://arxiv.org/abs/1805.04777)]  [[Code](https://github.com/MarvinTeichmann/ConvCRF)]
- Did the Model Understand the Question? [[arXiv](https://arxiv.org/abs/1805.05492)] [[Code](https://github.com/pramodkaushik/acl18_results)]
- From Word to Sense Embeddings: A Survey on Vector Representations of Meaning [[arXiv](https://arxiv.org/abs/1805.04032v2)]
- Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review [[arXiv](https://arxiv.org/abs/1805.00909)]

#### 2018-04
- The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation [[arXiv](https://arxiv.org/abs/1804.09849v2)]
- Linguistically-Informed Self-Attention for Semantic Role Labeling [[arXiv](https://arxiv.org/abs/1804.08199)]
- Taskonomy: Disentangling Task Transfer Learning [[arXiv](https://arxiv.org/abs/1804.08328)] [[Code](https://github.com/StanfordVL/taskonomy)]
- Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation [[arXiv](https://arxiv.org/abs/1804.08069)] [[Code](https://github.com/snakeztc/NeuralDialog-LAED)]
- Learning Semantic Textual Similarity from Conversations [[arXiv](https://arxiv.org/abs/1804.07754)]
- When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation? [[arXiv](https://arxiv.org/abs/1804.06323v2)]
- QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension [[arXiv](https://arxiv.org/abs/1804.09541v1)] [[Code](https://github.com/hengruo/QANet-pytorch)]
- Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models [[arXiv](https://arxiv.org/abs/1804.09299)] [[Code](https://github.com/HendrikStrobelt/Seq2Seq-Vis)]
- Low Rank Structure of Learned Representations [[arXiv](https://arxiv.org/abs/1804.07090)]
- Decoupled Networks [[arXiv](https://arxiv.org/abs/1804.08071)]
- Learned Deformation Stability in Convolutional Neural Networks [[arXiv](https://arxiv.org/abs/1804.04438)]
- Phrase-Based & Neural Unsupervised Machine Translation [[arXiv](https://arxiv.org/abs/1804.07755)]
- Learning to Map Context-Dependent Sentences to Executable Formal Queries [[arXiv](https://arxiv.org/abs/1804.06868)] [[Code](https://github.com/clic-lab/atis)]
- Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations [[arXiv](https://arxiv.org/abs/1804.02485)]
- Differentiable plasticity: training plastic neural networks with backpropagation [[arXiv](https://arxiv.org/abs/1804.02464)]
- Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning [[arXiv](https://arxiv.org/abs/1804.00079v1)] [[Code](https://github.com/Maluuba/gensen)]

#### 2018-03
- An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling [[arXiv](https://arxiv.org/abs/1803.01271v2)] [[Code](https://github.com/locuslab/TCN)]
- Variance Networks: When Expectation Does Not Meet Your Expectations [[arXiv](https://arxiv.org/abs/1803.03764)]
- Referring Relationships [[arXiv](https://arxiv.org/abs/1803.10362)]
- Iterative Visual Reasoning Beyond Convolutions [[arXiv](https://arxiv.org/abs/1803.11189)]
- World Models [[arXiv](https://arxiv.org/abs/1803.10122)]
- Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning [[arXiv](https://arxiv.org/abs/1803.05268)]
- A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay [[arXiv](https://arxiv.org/abs/1803.09820)]
- Feudal Reinforcement Learning for Dialogue Management in Large Domains [[arXiv](https://arxiv.org/abs/1803.03232)]
- Universal Sentence Encoder [[arXiv](https://arxiv.org/abs/1803.11175v1)]
- Averaging Weights Leads to Wider Optima and Better Generalization [[arXiv](https://arxiv.org/abs/1803.05407v1)]
- On the importance of single directions for generalization [[arXiv](https://arxiv.org/abs/1803.06959)]
- Group Normalization [[arXiv](https://arxiv.org/abs/1803.08494)]
- Compositional Attention Networks for Machine Reasoning [[arXiv](https://arxiv.org/abs/1803.03067)]
- Learning Longer-term Dependencies in RNNs with Auxiliary Losses [[arXiv](https://arxiv.org/abs/1803.00144)]

#### 2018-02
- Regularized Evolution for Image Classifier Architecture Search [[arXiv](https://arxiv.org/abs/1802.01548)]
- Visual Interpretability for Deep Learning: a Survey [[arXiv](https://arxiv.org/abs/1802.00614)]
- Deep contextualized word representation [[arXiv](https://arxiv.org/abs/1802.05365v2)]
- Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs [[arXiv](https://arxiv.org/abs/1802.10026)]
- Machine Theory of Mind [[arXiv](https://arxiv.org/abs/1802.07740)] 
- Efficient Neural Architecture Search via Parameter Sharing [[arXiv](https://arxiv.org/abs/1802.03268)]
- Interpreting CNNs via Decision Trees [[arXiv](https://arxiv.org/abs/1802.00121)]
- DensePose: Dense Human Pose Estimation In The Wild [[arXiv](https://arxiv.org/abs/1802.00434)] [[dataset](http://densepose.org/)]
- DeepType: Multilingual Entity Linking by Neural Type System Evolution [[arXiv](https://arxiv.org/abs/1802.01021)] [[Code](https://github.com/openai/deeptype)]
- Recent Advances in Neural Program Synthesis [[arXiv](https://arxiv.org/abs/1802.02353)]
- IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [[arXiv](https://arxiv.org/abs/1802.01561)] [[Code](https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/dmlab30)]
- Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples [[arXiv](https://arxiv.org/abs/1802.00420)] [[Code](https://github.com/anishathalye/obfuscated-gradients)]

#### 2018-01
- Universal Language Model Fine-tuning for Text Classification [[arXiv](https://arxiv.org/abs/1801.06146v5)]
- Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling [[arXiv](https://arxiv.org/abs/1801.10296)]
- Quantum Computing in the NISQ era and beyond [[arXiv](https://arxiv.org/abs/1801.00862)]
- Active Neural Localization [[arXiv](https://arxiv.org/abs/1801.08214)]
- Global overview of Imitation Learning [[arXiv](https://arxiv.org/abs/1801.06503)]
- MaskGAN: Better Text Generation via Filling in the ______ [[arXiv](https://arxiv.org/abs/1801.07736)] [[Code](https://github.com/tensorflow/models/tree/master/research/maskgan)]
- Deep Learning for Sentiment Analysis : A Survey [[arXiv](https://arxiv.org/abs/1801.07883)]
- Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift [[arXiv](https://arxiv.org/abs/1801.05134)]
- DENSER: Deep Evolutionary Network Structured Representation [[arXiv](https://arxiv.org/abs/1801.01563)]
- Building a Conversational Agent Overnight with Dialogue Self-Play [[arXiv](https://arxiv.org/abs/1801.04871)]
- Unsupervised Real-to-Virtual Domain Unification for End-to-End Highway Driving [[arXiv](https://arxiv.org/abs/1801.03458)]
- SBNet: Sparse Blocks Network for Fast Inference [[arXiv](https://arxiv.org/abs/1801.02108)]
- Deep Stereo Matching with Explicit Cost Aggregation Sub-Architecture [[arXiv](https://arxiv.org/abs/1801.04065)]
- Adversarial Spheres [[arXiv](https://arxiv.org/abs/1801.02774)]
- Neural Program Synthesis with Priority Queue Training [[arXiv](https://arxiv.org/abs/1801.03526)] [[Code](https://github.com/tensorflow/models/tree/master/research/brain_coder)]
- Adversarial Generative Nets: Neural Network Attacks on State-of-the-Art Face Recognition [[arXiv](https://arxiv.org/abs/1801.00349)]
- DeepMind Control Suite [[arXiv](https://arxiv.org/abs/1801.00690)] [[dataset](https://github.com/deepmind/dm_control)]

#### 2017-12
- Noisy Natural Gradient as Variational Inference [[arXiv](https://arxiv.org/abs/1712.02390)] [[Code](https://github.com/wlwkgus/NoisyNaturalGradient)]
- Non-convex Optimization for Machine Learning [[arXiv](https://arxiv.org/abs/1712.07897)]
- Improving Generalization Performance by Switching from Adam to SGD [[arXiv](https://arxiv.org/abs/1712.07628v1)]
- Learning by Asking Questions [[arXiv](https://arxiv.org/abs/1712.01238v1)]
- A Flexible Approach to Automated RNN Architecture Generation [[arXiv](https://arxiv.org/abs/1712.07316v1)]
- Lectures on Randomized Numerical Linear Algebra [[arXiv](https://arxiv.org/abs/1712.08880)]
- Data Distillation: Towards Omni-Supervised Learning [[arXiv](https://arxiv.org/abs/1712.04440)]
- Deep Learning Scaling is Predictable, Empirically [[arXiv](https://arxiv.org/abs/1712.00409v1)]
- SGAN: An Alternative Training of Generative Adversarial Networks [[arXiv](https://arxiv.org/abs/1712.02330v1)]
- The NarrativeQA Reading Comprehension Challenge [[arXiv](https://arxiv.org/abs/1712.07040v1)] [[dataset](https://github.com/deepmind/narrativeqa)]
- Regularization and Optimization strategies in Deep Convolutional Neural Network [[arXiv](https://arxiv.org/abs/1712.04711)]
- Mathematics of Deep Learning [[arXiv](https://arxiv.org/abs/1712.04741)]
- Text Generation Based on Generative Adversarial Nets with Latent Variable [[arXiv](https://arxiv.org/abs/1712.00170v1)]
- Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
- Show-and-Fool: Crafting Adversarial Examples for Neural Image Captioning [[arXiv](https://arxiv.org/abs/1712.02051)] [[Code](https://github.com/huanzhang12/ImageCaptioningAttack)]

#### ICLR-18
- Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling
- i-RevNet: Deep Invertible Networks
- Hierarchical Representations for Efficient Architecture Search [[arXiv](https://arxiv.org/abs/1711.00436)]
- Backpropagation through the Void: Optimizing control variates for black-box gradient estimation [[arXiv](https://arxiv.org/abs/1711.00123)] [[Code](https://github.com/duvenaud/relax)]
- Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection
- Wasserstein Auto-Encoders [[arXiv](https://arxiv.org/abs/1711.01558)]
- Neural Speed Reading via Skim-RNN [[arXiv](https://arxiv.org/abs/1711.02085)]
- Breaking the Softmax Bottleneck: A High-Rank RNN Language Model [[arXiv](https://arxiv.org/abs/1711.03953)] [[Code](https://github.com/zihangdai/mos)]
- Non-Autoregressive Neural Machine Translation [[arXiv](https://arxiv.org/abs/1711.02281)]
- A DIRT-T Approach to Unsupervised Domain Adaptation
- On the Information Bottleneck Theory of Deep Learning
- mixup: Beyond Empirical Risk Minimization
- Unsupervised Machine Translation Using Monolingual Corpora Only [[arXiv](https://arxiv.org/abs/1711.00043)]
- Matrix capsules with EM routing

#### 2017-11
- Hierarchical Representations for Efficient Architecture Search [[arXiv](https://arxiv.org/abs/1711.00436v2)]
- Embedding Words as Distributions with a Bayesian Skip-gram Model [[arXiv](https://arxiv.org/abs/1711.11027v1)]
- Deep Image Prior [[arXiv](https://arxiv.org/abs/1711.10925)] [[Code](https://github.com/DmitryUlyanov/deep-image-prior)]
- MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1711.06788v1)]
- Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning [[arXiv](https://arxiv.org/abs/1711.07613v1)]
- Neural Text Generation: A Practical Guide [[arXiv](https://arxiv.org/abs/1711.09534v1)]
- Memory Aware Synapses: Learning what (not) to forget [[arXiv](https://arxiv.org/abs/1711.09601)]
- Are GANs Created Equal? A Large-Scale Study [[arXiv](https://arxiv.org/abs/1711.10337)]
- Distilling a Neural Network Into a Soft Decision Tree [[arXiv](https://arxiv.org/abs/1711.09784)]
- SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability [[arXiv](https://arxiv.org/abs/1706.05806)] [[Code](https://github.com/google/svcca)]
- Non-local Neural Networks [[arXiv](https://arxiv.org/abs/1711.07971v1)]
- Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations [[arXiv](https://arxiv.org/abs/1711.05732)] [[dataset](https://github.com/jwieting)] [[Code](https://github.com/jwieting)]
- Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks [[arXiv](https://arxiv.org/abs/1711.00350)] [[dataset](https://github.com/brendenlake/SCAN)]
- Neural Discrete Representation Learning [[arXiv](https://arxiv.org/abs/1711.00937)]
- Weighted Transformer Network for Machine Translation [[arXiv](https://arxiv.org/abs/1711.02132)]

#### 2017-10
- Dynamic Routing Between Capsules [[arXiv](https://arxiv.org/abs/1710.09829)]
- Unsupervised Neural Machine Translation [[arXiv](https://arxiv.org/abs/1710.11041)]

#### 2017-09
- Empower Sequence Labeling with Task-Aware Neural Language Model [[arXiv](https://arxiv.org/abs/1709.04109)] [[Code](https://github.com/LiyuanLucasLiu/LM-LSTM-CRF)]
- Dynamic Evaluation of Neural Sequence Models [[arXiv](https://arxiv.org/abs/1709.07432)] [[Code](https://github.com/benkrause/dynamic-evaluation)]

#### NIPS-17
- Improved Training of Wasserstein GANs [[arXiv](https://arxiv.org/abs/1704.00028)] [[Code](https://github.com/igul222/improved_wgan_training)] [[Code](https://github.com/caogang/wgan-gp)]
- The Reversible Residual Network: Backpropagation Without Storing Activations [[arXiv](https://arxiv.org/abs/1707.04585)] [[Code](https://github.com/renmengye/revnet-public)] [[Code](https://github.com/tbung/pytorch-revnet)]
- Plan, Attend, Generate: Planning for Sequence-to-Sequence Models [[arXiv](https://arxiv.org/abs/1711.10462v1)]
- REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models [[arXiv](https://arxiv.org/abs/1703.07370)] [[Code](https://github.com/tensorflow/models/tree/master/research/rebar)]
- Poincaré Embeddings for Learning Hierarchical Representations [[arXiv](https://arxiv.org/abs/1705.08039)]
- Character-Level Language Modeling with Recurrent Highway Hypernetworks [[Code](https://github.com/jsuarez5341/Recurrent-Highway-Hypernetworks-NIPS)] 
- Dilated Recurrent Neural Networks [[arXiv](https://arxiv.org/abs/1710.02224)] [[Code](https://github.com/code-terminator/DilatedRNN)]
- Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning [[arXiv](https://arxiv.org/abs/1711.01577)]
- Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations [[arXiv](https://arxiv.org/abs/1704.00648)]
- Learned in Translation: Contextualized Word Vectors [[arXiv](https://arxiv.org/abs/1708.00107)] [[Code](https://github.com/salesforce/cove)]

#### 2017-08
- Focal Loss for Dense Object Detection [[arXiv](https://arxiv.org/abs/1708.02002)]
- Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge [[arXiv](https://arxiv.org/abs/1708.02711)] [[Code](https://github.com/hengyuan-hu/bottom-up-attention-vqa)]

#### 2017-07
- Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering [[arXiv](https://arxiv.org/abs/1707.07998)] [[Code](https://github.com/hengyuan-hu/bottom-up-attention-vqa)]

#### 2017-06
- Self-Normalizing Neural Networks [[arXiv](https://arxiv.org/abs/1706.02515)]
- One Model To Learn Them All [[arXiv](https://arxiv.org/abs/1706.05137)] [[Code](https://github.com/tensorflow/tensor2tensor)]
- Attention Is All You Need [[arXiv](https://arxiv.org/abs/1706.03762)] [[Code](https://github.com/tensorflow/tensor2tensor)]
- A simple neural network module for relational reasoning [[arXiv](https://arxiv.org/abs/1706.01427)]

#### ICML-17
- Large-Scale Evolution of Image Classifiers [[arXiv](https://arxiv.org/abs/1703.01041)]
- Improved Variational Autoencoders for Text Modeling using Dilated Convolutions [[arXiv](https://arxiv.org/abs/1702.08139)]
- Conditional Image Synthesis With Auxiliary Classifier GANs [[arXiv](https://arxiv.org/abs/1610.09585)]
- Toward Controlled Generation of Text [[arXiv](https://arxiv.org/abs/1703.00955)]

#### 2017-05
- Ask the Right Questions: Active Question Reformulation with Reinforcement Learning [[arXiv](https://arxiv.org/abs/1705.07830)]
- Supervised Learning of Universal Sentence Representations from Natural Language Inference Data [[arXiv](https://arxiv.org/abs/1705.02364)] [[Code](https://github.com/facebookresearch/InferSent)] [[Code](https://github.com/facebookresearch/SentEval)]
- Learning to Ask: Neural Question Generation for Reading Comprehension [[arXiv](https://arxiv.org/abs/1705.00106)] [[Code](https://github.com/xinyadu/nqg)]
- Adversarial Ranking for Language Generation [[arXiv](https://arxiv.org/abs/1705.11001)]
- Learning Structured Text Representations [[arXiv](https://arxiv.org/abs/1705.09207)] [[Code](https://github.com/nlpyang/structured)]
- TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension [[arXiv](https://arxiv.org/abs/1705.03551)] [[dataset](http://nlp.cs.washington.edu/triviaqa/)] [[Code](https://github.com/mandarjoshi90/triviaqa)]
- ParlAI: A Dialog Research Software Platform [[arXiv](https://arxiv.org/abs/1705.06476)] [[Code](https://github.com/facebookresearch/ParlAI)]

#### 2017-04
- Stochastic Gradient Descent as Approximate Bayesian Inference [[arXiv](https://arxiv.org/abs/1704.04289)]
- Snapshot Ensembles: Train 1, get M for free [[arXiv](https://arxiv.org/abs/1704.00109)] [[Code](https://github.com/gaohuang/SnapshotEnsemble)] [[Code](https://github.com/titu1994/Snapshot-Ensembles)]
- From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood [[arXiv](https://arxiv.org/abs/1704.07926)] [[Code](https://github.com/kelvinguu/lang2program)]
- Reading Wikipedia to Answer Open-Domain Questions [[arXiv](https://arxiv.org/abs/1704.00051)]
- Learning to Skim Text [[arXiv](https://arxiv.org/abs/1704.06877)]

#### 2017-03
- Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [[arXiv](https://arxiv.org/abs/1703.10593)] [[Code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)]
- Evolution Strategies as a Scalable Alternative to Reinforcement Learning [[arXiv](https://arxiv.org/abs/1703.03864)] [[Code](https://github.com/openai/evolution-strategies-starter)] [[Code](https://github.com/atgambardella/pytorch-es)]

#### 2017-02
- Exploring loss function topology with cyclical learning rates [[arXiv](https://arxiv.org/abs/1702.04283)] [[Code](https://github.com/lnsmith54/exploring-loss)]
- A Hybrid Convolutional Variational Autoencoder for Text Generation [[arXiv](https://arxiv.org/abs/1702.02390)] [[Code](https://github.com/stas-semeniuta/textvae)]

#### 2017-01
- Wasserstein GAN [[arXiv](https://arxiv.org/abs/1701.07875)] [[Code](https://github.com/martinarjovsky/WassersteinGAN)]
- Adversarial Learning for Neural Dialogue Generation [[arXiv](https://arxiv.org/abs/1701.06547)]
- Deep Reinforcement Learning: An Overview [[arXiv](https://arxiv.org/abs/1701.07274)]
- OpenNMT: Open-Source Toolkit for Neural Machine Translation [[arXiv](https://arxiv.org/abs/1701.02810)] [[Code](https://github.com/OpenNMT/OpenNMT)] [[Code](https://github.com/OpenNMT/OpenNMT-py)]

#### ICLR-17
- Categorical Reparameterization with Gumbel-Softmax [[arXiv](https://arxiv.org/abs/1611.01144)]
- Adversarial Training Methods for Semi-Supervised Text Classification [[arXiv](https://arxiv.org/abs/1605.07725)] [[Code](https://github.com/tensorflow/models/tree/master/research/adversarial_text)]
- Structured Attention Networks [[arXiv](https://arxiv.org/abs/1702.00887)] [[Code](https://github.com/harvardnlp/struct-attn)]
- Learning End-to-End Goal-Oriented Dialog [[arXiv](https://arxiv.org/abs/1605.07683)] [[dataset](http://fb.ai/babi)]
- Understanding deep learning requires rethinking generalization [[arXiv](https://arxiv.org/abs/1611.03530)]
- An Actor-Critic Algorithm for Sequence Prediction [[arXiv](https://arxiv.org/abs/1607.07086)]
- Learning to Remember Rare Events [[arXiv](https://arxiv.org/abs/1703.03129)]
- Introspection: Accelerating Neural Network Training By Learning Weight Evolution [[arXiv](https://arxiv.org/abs/1704.04959)]

#### 2016-11
- Image-to-Image Translation with Conditional Adversarial Networks [[arXiv](https://arxiv.org/abs/1611.07004v2)] [[Code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)]

#### 2016-10
- Using Fast Weights to Attend to the Recent Past [[arXiv](https://arxiv.org/abs/1610.06258)]

#### 2016-09
- HyperNetworks [[arXiv](https://arxiv.org/abs/1609.09106)]
- Language as a Latent Variable: Discrete Generative Models for Sentence Compression [[arXiv](https://arxiv.org/abs/1609.07317)]
- SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient [[arXiv](https://arxiv.org/abs/1609.05473)] [[Code](https://github.com/LantaoYu/SeqGAN)]

#### 2016-08
- SGDR: Stochastic Gradient Descent with Warm Restarts [[arXiv](https://arxiv.org/abs/1608.03983)] [[Code](https://github.com/loshchil/SGDR)]

#### 2016-06
- Convolution by Evolution: Differentiable Pattern Producing Networks [[arXiv](https://arxiv.org/abs/1606.02580)]
- Conditional Image Generation with PixelCNN Decoders [[arXiv](https://arxiv.org/abs/1606.05328)]
- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets [[arXiv](https://arxiv.org/abs/1606.03657)]

#### 2016-04
- Benchmarking Deep Reinforcement Learning for Continuous Control [[arXiv](https://arxiv.org/abs/1604.06778)] [[Code](https://github.com/rll/rllab)]

#### 2016-02
- Associative Long Short-Term Memory [[arXiv](https://arxiv.org/abs/1602.03032)]

#### 2015-11
- Neural Variational Inference for Text Processing [[arXiv](https://arxiv.org/abs/1511.06038)]
- Generating Sentences from a Continuous Space [[arXiv](https://arxiv.org/abs/1511.06349)]

#### 2015-06
- Cyclical Learning Rates for Training Neural Networks [[arXiv](https://arxiv.org/abs/1506.01186)] [[Code](https://github.com/bckenstler/CLR)]
- Skip-Thought Vectors [[arXiv](https://arxiv.org/abs/1506.06726)]

#### 2015-05
- U-Net: Convolutional Networks for Biomedical Image Segmentation [[arXiv](https://arxiv.org/abs/1505.04597)]

#### 2015-02
- Trust Region Policy Optimization [[arXiv](https://arxiv.org/abs/1502.05477)]

#### 2014-12
- Adam: A Method for Stochastic Optimization [[arXiv](https://arxiv.org/abs/1412.6980)]

#### 2014-11
- Conditional Generative Adversarial Nets [[arXiv](https://arxiv.org/abs/1411.1784)]

#### 2014-06
- Generative Adversarial Networks [[arXiv](https://arxiv.org/abs/1406.2661)]
- Semi-Supervised Learning with Deep Generative Models [[arXiv](https://arxiv.org/abs/1406.5298)] [[Code](https://github.com/dpkingma/nips14-ssl)]

#### 2013-12
- Auto-Encoding Variational Bayes [[arXiv](https://arxiv.org/abs/1312.6114)]

#### 2012-06
- Variational Bayesian Inference with Stochastic Search [[arXiv](https://arxiv.org/abs/1206.6430)]
